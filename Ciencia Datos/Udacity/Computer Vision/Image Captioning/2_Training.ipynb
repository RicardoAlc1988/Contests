{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** The CNN-RNN is composed of two parts:\n",
    "- It has been proved that the last layer of a well trained convolutional architecture represents accurately the image features. Taking this idea, the first component of the architecture consists of an frozen CNN architecture (meaning is no longer trained in the backpropagation process) with the adition of an embedding representation of the image, by applying a fully connected layer. This representation is the input of the decoder.\n",
    "\n",
    "\n",
    "- The decoder takes as input the image representation as an embedding along with the embedding representation of the sentences. The embeddings has been proved to represent very well semantic relationships between the words. This input is used by the LSTM to modelize the sequential nature of the sentences and, finally, in the top of the LSTM a fully connected layer is applied to calculate the scores, meaning the propabilies of a word, given the image and previous words of the sentence.\n",
    "\n",
    "As suggested, we take the ideas for the architecture from this paper: https://arxiv.org/pdf/1411.4555.pdf\n",
    "\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** Apart from normalizing, the trasformations has as a goal to achieve generalization. In this sense, I think the provided transformations are reasonable (crop and flip). Perhaps another transformation colub be provided, i.e., translation. \n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** The CNN Encoder has the convolutional part frozen (transfer learning), so only was necessary to train the parto to obtain the embedding representation. \n",
    "\n",
    "With respect to the decoder, no transfer learning technique was applied, so it is needed to train the whole architecture. As a possible alternative, transfer learning could be used for the embbedding word part, but, as it was suggested in the paper, no substantial improve is achieved.\n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** We used the current state of the art ADAM optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/414113 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 415/414113 [00:00<01:39, 4147.52it/s]\u001b[A\n",
      "  0%|          | 877/414113 [00:00<01:36, 4277.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.89s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "  0%|          | 1330/414113 [00:00<01:34, 4347.59it/s]\u001b[A\n",
      "  0%|          | 1781/414113 [00:00<01:33, 4394.46it/s]\u001b[A\n",
      "  1%|          | 2237/414113 [00:00<01:32, 4442.65it/s]\u001b[A\n",
      "  1%|          | 2696/414113 [00:00<01:31, 4483.67it/s]\u001b[A\n",
      "  1%|          | 3097/414113 [00:00<01:36, 4279.07it/s]\u001b[A\n",
      "  1%|          | 3547/414113 [00:00<01:34, 4340.95it/s]\u001b[A\n",
      "  1%|          | 3982/414113 [00:00<01:34, 4341.87it/s]\u001b[A\n",
      "  1%|          | 4449/414113 [00:01<01:32, 4432.34it/s]\u001b[A\n",
      "  1%|          | 4912/414113 [00:01<01:31, 4488.84it/s]\u001b[A\n",
      "  1%|▏         | 5380/414113 [00:01<01:29, 4544.15it/s]\u001b[A\n",
      "  1%|▏         | 5832/414113 [00:01<01:29, 4536.65it/s]\u001b[A\n",
      "  2%|▏         | 6283/414113 [00:01<01:30, 4527.48it/s]\u001b[A\n",
      "  2%|▏         | 6734/414113 [00:01<01:30, 4514.65it/s]\u001b[A\n",
      "  2%|▏         | 7184/414113 [00:01<01:30, 4490.67it/s]\u001b[A\n",
      "  2%|▏         | 7633/414113 [00:01<01:30, 4489.59it/s]\u001b[A\n",
      "  2%|▏         | 8088/414113 [00:01<01:30, 4506.96it/s]\u001b[A\n",
      "  2%|▏         | 8540/414113 [00:01<01:29, 4509.32it/s]\u001b[A\n",
      "  2%|▏         | 9000/414113 [00:02<01:29, 4534.30it/s]\u001b[A\n",
      "  2%|▏         | 9454/414113 [00:02<01:29, 4518.24it/s]\u001b[A\n",
      "  2%|▏         | 9906/414113 [00:02<01:29, 4513.17it/s]\u001b[A\n",
      "  3%|▎         | 10358/414113 [00:02<01:29, 4508.27it/s]\u001b[A\n",
      "  3%|▎         | 10812/414113 [00:02<01:29, 4516.55it/s]\u001b[A\n",
      "  3%|▎         | 11270/414113 [00:02<01:28, 4534.63it/s]\u001b[A\n",
      "  3%|▎         | 11729/414113 [00:02<01:28, 4549.33it/s]\u001b[A\n",
      "  3%|▎         | 12198/414113 [00:02<01:27, 4588.14it/s]\u001b[A\n",
      "  3%|▎         | 12657/414113 [00:02<01:28, 4540.45it/s]\u001b[A\n",
      "  3%|▎         | 13112/414113 [00:02<01:28, 4534.22it/s]\u001b[A\n",
      "  3%|▎         | 13568/414113 [00:03<01:28, 4539.95it/s]\u001b[A\n",
      "  3%|▎         | 14034/414113 [00:03<01:27, 4575.10it/s]\u001b[A\n",
      "  4%|▎         | 14504/414113 [00:03<01:26, 4609.39it/s]\u001b[A\n",
      "  4%|▎         | 14966/414113 [00:03<01:26, 4592.24it/s]\u001b[A\n",
      "  4%|▎         | 15428/414113 [00:03<01:26, 4599.98it/s]\u001b[A\n",
      "  4%|▍         | 15889/414113 [00:03<01:27, 4552.17it/s]\u001b[A\n",
      "  4%|▍         | 16345/414113 [00:03<01:28, 4515.54it/s]\u001b[A\n",
      "  4%|▍         | 16801/414113 [00:03<01:27, 4528.78it/s]\u001b[A\n",
      "  4%|▍         | 17255/414113 [00:03<01:27, 4525.93it/s]\u001b[A\n",
      "  4%|▍         | 17708/414113 [00:03<01:27, 4524.84it/s]\u001b[A\n",
      "  4%|▍         | 18161/414113 [00:04<01:27, 4517.08it/s]\u001b[A\n",
      "  4%|▍         | 18613/414113 [00:04<01:27, 4507.65it/s]\u001b[A\n",
      "  5%|▍         | 19064/414113 [00:04<01:27, 4496.23it/s]\u001b[A\n",
      "  5%|▍         | 19516/414113 [00:04<01:27, 4502.33it/s]\u001b[A\n",
      "  5%|▍         | 19971/414113 [00:04<01:27, 4515.35it/s]\u001b[A\n",
      "  5%|▍         | 20429/414113 [00:04<01:26, 4532.94it/s]\u001b[A\n",
      "  5%|▌         | 20890/414113 [00:04<01:26, 4555.11it/s]\u001b[A\n",
      "  5%|▌         | 21357/414113 [00:04<01:25, 4587.12it/s]\u001b[A\n",
      "  5%|▌         | 21819/414113 [00:04<01:25, 4594.98it/s]\u001b[A\n",
      "  5%|▌         | 22279/414113 [00:04<01:25, 4561.21it/s]\u001b[A\n",
      "  5%|▌         | 22743/414113 [00:05<01:25, 4583.13it/s]\u001b[A\n",
      "  6%|▌         | 23202/414113 [00:05<01:25, 4573.85it/s]\u001b[A\n",
      "  6%|▌         | 23660/414113 [00:05<01:25, 4551.18it/s]\u001b[A\n",
      "  6%|▌         | 24117/414113 [00:05<01:25, 4555.57it/s]\u001b[A\n",
      "  6%|▌         | 24582/414113 [00:05<01:25, 4582.40it/s]\u001b[A\n",
      "  6%|▌         | 25054/414113 [00:05<01:24, 4619.87it/s]\u001b[A\n",
      "  6%|▌         | 25534/414113 [00:05<01:23, 4671.54it/s]\u001b[A\n",
      "  6%|▋         | 26002/414113 [00:05<01:24, 4593.86it/s]\u001b[A\n",
      "  6%|▋         | 26465/414113 [00:05<01:24, 4603.67it/s]\u001b[A\n",
      "  7%|▋         | 26926/414113 [00:05<01:24, 4586.80it/s]\u001b[A\n",
      "  7%|▋         | 27385/414113 [00:06<01:24, 4564.68it/s]\u001b[A\n",
      "  7%|▋         | 27860/414113 [00:06<01:23, 4616.62it/s]\u001b[A\n",
      "  7%|▋         | 28322/414113 [00:06<01:24, 4592.59it/s]\u001b[A\n",
      "  7%|▋         | 28786/414113 [00:06<01:23, 4606.58it/s]\u001b[A\n",
      "  7%|▋         | 29250/414113 [00:06<01:23, 4615.96it/s]\u001b[A\n",
      "  7%|▋         | 29721/414113 [00:06<01:22, 4640.85it/s]\u001b[A\n",
      "  7%|▋         | 30193/414113 [00:06<01:22, 4662.45it/s]\u001b[A\n",
      "  7%|▋         | 30660/414113 [00:06<01:22, 4646.67it/s]\u001b[A\n",
      "  8%|▊         | 31132/414113 [00:06<01:22, 4667.29it/s]\u001b[A\n",
      "  8%|▊         | 31599/414113 [00:06<01:22, 4645.53it/s]\u001b[A\n",
      "  8%|▊         | 32068/414113 [00:07<01:22, 4656.35it/s]\u001b[A\n",
      "  8%|▊         | 32534/414113 [00:07<01:22, 4648.49it/s]\u001b[A\n",
      "  8%|▊         | 32999/414113 [00:07<01:23, 4591.11it/s]\u001b[A\n",
      "  8%|▊         | 33459/414113 [00:07<01:23, 4572.64it/s]\u001b[A\n",
      "  8%|▊         | 33917/414113 [00:07<01:23, 4570.66it/s]\u001b[A\n",
      "  8%|▊         | 34386/414113 [00:07<01:22, 4603.00it/s]\u001b[A\n",
      "  8%|▊         | 34847/414113 [00:07<01:22, 4599.44it/s]\u001b[A\n",
      "  9%|▊         | 35308/414113 [00:07<01:22, 4589.59it/s]\u001b[A\n",
      "  9%|▊         | 35768/414113 [00:07<01:22, 4560.25it/s]\u001b[A\n",
      "  9%|▊         | 36225/414113 [00:07<01:23, 4533.77it/s]\u001b[A\n",
      "  9%|▉         | 36695/414113 [00:08<01:22, 4580.98it/s]\u001b[A\n",
      "  9%|▉         | 37154/414113 [00:08<01:25, 4383.89it/s]\u001b[A\n",
      "  9%|▉         | 37618/414113 [00:08<01:24, 4456.08it/s]\u001b[A\n",
      "  9%|▉         | 38083/414113 [00:08<01:23, 4512.51it/s]\u001b[A\n",
      "  9%|▉         | 38544/414113 [00:08<01:22, 4539.79it/s]\u001b[A\n",
      "  9%|▉         | 39002/414113 [00:08<01:22, 4551.49it/s]\u001b[A\n",
      " 10%|▉         | 39463/414113 [00:08<01:22, 4566.31it/s]\u001b[A\n",
      " 10%|▉         | 39921/414113 [00:08<01:22, 4546.53it/s]\u001b[A\n",
      " 10%|▉         | 40377/414113 [00:08<01:22, 4515.74it/s]\u001b[A\n",
      " 10%|▉         | 40837/414113 [00:08<01:22, 4539.61it/s]\u001b[A\n",
      " 10%|▉         | 41312/414113 [00:09<01:21, 4600.18it/s]\u001b[A\n",
      " 10%|█         | 41775/414113 [00:09<01:20, 4608.95it/s]\u001b[A\n",
      " 10%|█         | 42237/414113 [00:09<01:20, 4598.88it/s]\u001b[A\n",
      " 10%|█         | 42698/414113 [00:09<01:21, 4581.98it/s]\u001b[A\n",
      " 10%|█         | 43157/414113 [00:09<01:22, 4493.17it/s]\u001b[A\n",
      " 11%|█         | 43610/414113 [00:09<01:22, 4502.05it/s]\u001b[A\n",
      " 11%|█         | 44067/414113 [00:09<01:21, 4520.48it/s]\u001b[A\n",
      " 11%|█         | 44525/414113 [00:09<01:21, 4536.36it/s]\u001b[A\n",
      " 11%|█         | 44979/414113 [00:09<01:21, 4509.97it/s]\u001b[A\n",
      " 11%|█         | 45431/414113 [00:09<01:21, 4506.85it/s]\u001b[A\n",
      " 11%|█         | 45889/414113 [00:10<01:21, 4526.08it/s]\u001b[A\n",
      " 11%|█         | 46351/414113 [00:10<01:20, 4551.46it/s]\u001b[A\n",
      " 11%|█▏        | 46807/414113 [00:10<01:21, 4522.86it/s]\u001b[A\n",
      " 11%|█▏        | 47260/414113 [00:10<01:21, 4489.82it/s]\u001b[A\n",
      " 12%|█▏        | 47710/414113 [00:10<01:22, 4461.72it/s]\u001b[A\n",
      " 12%|█▏        | 48171/414113 [00:10<01:21, 4503.93it/s]\u001b[A\n",
      " 12%|█▏        | 48632/414113 [00:10<01:20, 4533.36it/s]\u001b[A\n",
      " 12%|█▏        | 49086/414113 [00:10<01:20, 4508.58it/s]\u001b[A\n",
      " 12%|█▏        | 49538/414113 [00:10<01:20, 4503.62it/s]\u001b[A\n",
      " 12%|█▏        | 49994/414113 [00:11<01:20, 4518.22it/s]\u001b[A\n",
      " 12%|█▏        | 50467/414113 [00:11<01:19, 4576.82it/s]\u001b[A\n",
      " 12%|█▏        | 50927/414113 [00:11<01:19, 4583.30it/s]\u001b[A\n",
      " 12%|█▏        | 51386/414113 [00:11<01:19, 4553.15it/s]\u001b[A\n",
      " 13%|█▎        | 51842/414113 [00:11<01:19, 4549.80it/s]\u001b[A\n",
      " 13%|█▎        | 52303/414113 [00:11<01:19, 4567.36it/s]\u001b[A\n",
      " 13%|█▎        | 52760/414113 [00:11<01:19, 4531.88it/s]\u001b[A\n",
      " 13%|█▎        | 53214/414113 [00:11<01:21, 4421.96it/s]\u001b[A\n",
      " 13%|█▎        | 53663/414113 [00:11<01:21, 4440.61it/s]\u001b[A\n",
      " 13%|█▎        | 54123/414113 [00:11<01:20, 4484.82it/s]\u001b[A\n",
      " 13%|█▎        | 54572/414113 [00:12<01:20, 4471.45it/s]\u001b[A\n",
      " 13%|█▎        | 55020/414113 [00:12<01:22, 4349.63it/s]\u001b[A\n",
      " 13%|█▎        | 55496/414113 [00:12<01:20, 4464.20it/s]\u001b[A\n",
      " 14%|█▎        | 55957/414113 [00:12<01:19, 4504.41it/s]\u001b[A\n",
      " 14%|█▎        | 56414/414113 [00:12<01:19, 4523.83it/s]\u001b[A\n",
      " 14%|█▎        | 56881/414113 [00:12<01:18, 4565.99it/s]\u001b[A\n",
      " 14%|█▍        | 57350/414113 [00:12<01:17, 4602.49it/s]\u001b[A\n",
      " 14%|█▍        | 57811/414113 [00:12<01:17, 4604.17it/s]\u001b[A\n",
      " 14%|█▍        | 58272/414113 [00:12<01:17, 4586.43it/s]\u001b[A\n",
      " 14%|█▍        | 58731/414113 [00:12<01:17, 4559.71it/s]\u001b[A\n",
      " 14%|█▍        | 59204/414113 [00:13<01:17, 4606.71it/s]\u001b[A\n",
      " 14%|█▍        | 59685/414113 [00:13<01:15, 4664.60it/s]\u001b[A\n",
      " 15%|█▍        | 60161/414113 [00:13<01:15, 4692.74it/s]\u001b[A\n",
      " 15%|█▍        | 60631/414113 [00:13<01:15, 4664.78it/s]\u001b[A\n",
      " 15%|█▍        | 61098/414113 [00:13<01:15, 4651.58it/s]\u001b[A\n",
      " 15%|█▍        | 61564/414113 [00:13<01:15, 4647.31it/s]\u001b[A\n",
      " 15%|█▍        | 62029/414113 [00:13<01:15, 4645.32it/s]\u001b[A\n",
      " 15%|█▌        | 62494/414113 [00:13<01:16, 4618.55it/s]\u001b[A\n",
      " 15%|█▌        | 62959/414113 [00:13<01:15, 4625.02it/s]\u001b[A\n",
      " 15%|█▌        | 63422/414113 [00:13<01:16, 4590.51it/s]\u001b[A\n",
      " 15%|█▌        | 63882/414113 [00:14<01:16, 4570.59it/s]\u001b[A\n",
      " 16%|█▌        | 64340/414113 [00:14<01:16, 4566.65it/s]\u001b[A\n",
      " 16%|█▌        | 64797/414113 [00:14<01:16, 4538.13it/s]\u001b[A\n",
      " 16%|█▌        | 65251/414113 [00:14<01:17, 4515.31it/s]\u001b[A\n",
      " 16%|█▌        | 65716/414113 [00:14<01:16, 4552.43it/s]\u001b[A\n",
      " 16%|█▌        | 66192/414113 [00:14<01:15, 4610.60it/s]\u001b[A\n",
      " 16%|█▌        | 66656/414113 [00:14<01:15, 4614.19it/s]\u001b[A\n",
      " 16%|█▌        | 67118/414113 [00:14<01:16, 4541.74it/s]\u001b[A\n",
      " 16%|█▋        | 67577/414113 [00:14<01:16, 4554.54it/s]\u001b[A\n",
      " 16%|█▋        | 68033/414113 [00:14<01:16, 4523.01it/s]\u001b[A\n",
      " 17%|█▋        | 68486/414113 [00:15<01:17, 4487.95it/s]\u001b[A\n",
      " 17%|█▋        | 68948/414113 [00:15<01:16, 4526.58it/s]\u001b[A\n",
      " 17%|█▋        | 69402/414113 [00:15<01:16, 4529.63it/s]\u001b[A\n",
      " 17%|█▋        | 69856/414113 [00:15<01:16, 4504.55it/s]\u001b[A\n",
      " 17%|█▋        | 70307/414113 [00:15<03:15, 1760.57it/s]\u001b[A\n",
      " 17%|█▋        | 70708/414113 [00:16<02:42, 2116.51it/s]\u001b[A\n",
      " 17%|█▋        | 71176/414113 [00:16<02:15, 2532.10it/s]\u001b[A\n",
      " 17%|█▋        | 71646/414113 [00:16<01:56, 2938.23it/s]\u001b[A\n",
      " 17%|█▋        | 72117/414113 [00:16<01:43, 3311.09it/s]\u001b[A\n",
      " 18%|█▊        | 72575/414113 [00:16<01:34, 3609.51it/s]\u001b[A\n",
      " 18%|█▊        | 73048/414113 [00:16<01:27, 3883.25it/s]\u001b[A\n",
      " 18%|█▊        | 73497/414113 [00:16<01:24, 4041.18it/s]\u001b[A\n",
      " 18%|█▊        | 73967/414113 [00:16<01:20, 4218.17it/s]\u001b[A\n",
      " 18%|█▊        | 74422/414113 [00:16<01:19, 4291.98it/s]\u001b[A\n",
      " 18%|█▊        | 74877/414113 [00:16<01:17, 4363.76it/s]\u001b[A\n",
      " 18%|█▊        | 75341/414113 [00:17<01:16, 4442.27it/s]\u001b[A\n",
      " 18%|█▊        | 75806/414113 [00:17<01:15, 4501.80it/s]\u001b[A\n",
      " 18%|█▊        | 76268/414113 [00:17<01:14, 4535.11it/s]\u001b[A\n",
      " 19%|█▊        | 76746/414113 [00:17<01:13, 4604.00it/s]\u001b[A\n",
      " 19%|█▊        | 77211/414113 [00:17<01:13, 4614.54it/s]\u001b[A\n",
      " 19%|█▉        | 77682/414113 [00:17<01:12, 4640.60it/s]\u001b[A\n",
      " 19%|█▉        | 78151/414113 [00:17<01:12, 4653.85it/s]\u001b[A\n",
      " 19%|█▉        | 78618/414113 [00:17<01:12, 4624.31it/s]\u001b[A\n",
      " 19%|█▉        | 79082/414113 [00:17<01:12, 4617.85it/s]\u001b[A\n",
      " 19%|█▉        | 79545/414113 [00:17<01:12, 4610.80it/s]\u001b[A\n",
      " 19%|█▉        | 80036/414113 [00:18<01:11, 4695.69it/s]\u001b[A\n",
      " 19%|█▉        | 80520/414113 [00:18<01:10, 4735.37it/s]\u001b[A\n",
      " 20%|█▉        | 80997/414113 [00:18<01:10, 4743.98it/s]\u001b[A\n",
      " 20%|█▉        | 81472/414113 [00:18<01:10, 4706.22it/s]\u001b[A\n",
      " 20%|█▉        | 81943/414113 [00:18<01:11, 4616.82it/s]\u001b[A\n",
      " 20%|█▉        | 82406/414113 [00:18<01:12, 4599.19it/s]\u001b[A\n",
      " 20%|██        | 82867/414113 [00:18<01:12, 4578.95it/s]\u001b[A\n",
      " 20%|██        | 83335/414113 [00:18<01:11, 4606.98it/s]\u001b[A\n",
      " 20%|██        | 83796/414113 [00:18<01:12, 4580.80it/s]\u001b[A\n",
      " 20%|██        | 84263/414113 [00:19<01:11, 4605.97it/s]\u001b[A\n",
      " 20%|██        | 84728/414113 [00:19<01:11, 4616.87it/s]\u001b[A\n",
      " 21%|██        | 85205/414113 [00:19<01:10, 4660.00it/s]\u001b[A\n",
      " 21%|██        | 85672/414113 [00:19<01:10, 4634.70it/s]\u001b[A\n",
      " 21%|██        | 86145/414113 [00:19<01:10, 4662.17it/s]\u001b[A\n",
      " 21%|██        | 86618/414113 [00:19<01:09, 4681.60it/s]\u001b[A\n",
      " 21%|██        | 87087/414113 [00:19<01:11, 4595.44it/s]\u001b[A\n",
      " 21%|██        | 87559/414113 [00:19<01:10, 4629.15it/s]\u001b[A\n",
      " 21%|██▏       | 88026/414113 [00:19<01:10, 4641.31it/s]\u001b[A\n",
      " 21%|██▏       | 88491/414113 [00:19<01:10, 4638.48it/s]\u001b[A\n",
      " 21%|██▏       | 88967/414113 [00:20<01:09, 4673.80it/s]\u001b[A\n",
      " 22%|██▏       | 89441/414113 [00:20<01:09, 4691.87it/s]\u001b[A\n",
      " 22%|██▏       | 89913/414113 [00:20<01:09, 4698.13it/s]\u001b[A\n",
      " 22%|██▏       | 90388/414113 [00:20<01:08, 4713.39it/s]\u001b[A\n",
      " 22%|██▏       | 90860/414113 [00:20<01:08, 4693.30it/s]\u001b[A\n",
      " 22%|██▏       | 91336/414113 [00:20<01:08, 4709.86it/s]\u001b[A\n",
      " 22%|██▏       | 91808/414113 [00:20<01:08, 4708.17it/s]\u001b[A\n",
      " 22%|██▏       | 92284/414113 [00:20<01:08, 4722.53it/s]\u001b[A\n",
      " 22%|██▏       | 92757/414113 [00:20<01:08, 4687.98it/s]\u001b[A\n",
      " 23%|██▎       | 93226/414113 [00:20<01:09, 4628.46it/s]\u001b[A\n",
      " 23%|██▎       | 93690/414113 [00:21<01:09, 4604.28it/s]\u001b[A\n",
      " 23%|██▎       | 94166/414113 [00:21<01:08, 4647.80it/s]\u001b[A\n",
      " 23%|██▎       | 94632/414113 [00:21<01:08, 4630.51it/s]\u001b[A\n",
      " 23%|██▎       | 95096/414113 [00:21<01:08, 4629.30it/s]\u001b[A\n",
      " 23%|██▎       | 95560/414113 [00:21<01:08, 4627.35it/s]\u001b[A\n",
      " 23%|██▎       | 96023/414113 [00:21<01:09, 4586.17it/s]\u001b[A\n",
      " 23%|██▎       | 96487/414113 [00:21<01:09, 4600.68it/s]\u001b[A\n",
      " 23%|██▎       | 96959/414113 [00:21<01:08, 4633.49it/s]\u001b[A\n",
      " 24%|██▎       | 97423/414113 [00:21<01:08, 4591.06it/s]\u001b[A\n",
      " 24%|██▎       | 97883/414113 [00:21<01:08, 4592.06it/s]\u001b[A\n",
      " 24%|██▎       | 98343/414113 [00:22<01:09, 4554.19it/s]\u001b[A\n",
      " 24%|██▍       | 98807/414113 [00:22<01:08, 4577.21it/s]\u001b[A\n",
      " 24%|██▍       | 99271/414113 [00:22<01:08, 4594.38it/s]\u001b[A\n",
      " 24%|██▍       | 99731/414113 [00:22<01:08, 4595.95it/s]\u001b[A\n",
      " 24%|██▍       | 100191/414113 [00:22<01:08, 4584.20it/s]\u001b[A\n",
      " 24%|██▍       | 100650/414113 [00:22<01:08, 4561.94it/s]\u001b[A\n",
      " 24%|██▍       | 101111/414113 [00:22<01:08, 4575.99it/s]\u001b[A\n",
      " 25%|██▍       | 101569/414113 [00:22<01:08, 4545.50it/s]\u001b[A\n",
      " 25%|██▍       | 102026/414113 [00:22<01:08, 4550.20it/s]\u001b[A\n",
      " 25%|██▍       | 102482/414113 [00:22<01:08, 4540.69it/s]\u001b[A\n",
      " 25%|██▍       | 102945/414113 [00:23<01:08, 4564.29it/s]\u001b[A\n",
      " 25%|██▍       | 103418/414113 [00:23<01:07, 4610.95it/s]\u001b[A\n",
      " 25%|██▌       | 103880/414113 [00:23<01:07, 4577.30it/s]\u001b[A\n",
      " 25%|██▌       | 104343/414113 [00:23<01:07, 4590.58it/s]\u001b[A\n",
      " 25%|██▌       | 104803/414113 [00:23<01:07, 4569.83it/s]\u001b[A\n",
      " 25%|██▌       | 105268/414113 [00:23<01:07, 4591.01it/s]\u001b[A\n",
      " 26%|██▌       | 105728/414113 [00:23<01:09, 4467.63it/s]\u001b[A\n",
      " 26%|██▌       | 106189/414113 [00:23<01:08, 4509.31it/s]\u001b[A\n",
      " 26%|██▌       | 106641/414113 [00:23<01:08, 4497.16it/s]\u001b[A\n",
      " 26%|██▌       | 107092/414113 [00:23<01:11, 4312.22it/s]\u001b[A\n",
      " 26%|██▌       | 107540/414113 [00:24<01:10, 4358.75it/s]\u001b[A\n",
      " 26%|██▌       | 107982/414113 [00:24<01:09, 4374.66it/s]\u001b[A\n",
      " 26%|██▌       | 108432/414113 [00:24<01:09, 4411.52it/s]\u001b[A\n",
      " 26%|██▋       | 108884/414113 [00:24<01:08, 4441.34it/s]\u001b[A\n",
      " 26%|██▋       | 109329/414113 [00:24<01:08, 4423.25it/s]\u001b[A\n",
      " 27%|██▋       | 109783/414113 [00:24<01:08, 4455.77it/s]\u001b[A\n",
      " 27%|██▋       | 110247/414113 [00:24<01:07, 4507.45it/s]\u001b[A\n",
      " 27%|██▋       | 110701/414113 [00:24<01:07, 4516.89it/s]\u001b[A\n",
      " 27%|██▋       | 111155/414113 [00:24<01:06, 4523.10it/s]\u001b[A\n",
      " 27%|██▋       | 111611/414113 [00:24<01:06, 4532.71it/s]\u001b[A\n",
      " 27%|██▋       | 112065/414113 [00:25<01:07, 4500.97it/s]\u001b[A\n",
      " 27%|██▋       | 112516/414113 [00:25<01:07, 4490.44it/s]\u001b[A\n",
      " 27%|██▋       | 112968/414113 [00:25<01:06, 4498.95it/s]\u001b[A\n",
      " 27%|██▋       | 113426/414113 [00:25<01:06, 4521.02it/s]\u001b[A\n",
      " 27%|██▋       | 113881/414113 [00:25<01:06, 4528.34it/s]\u001b[A\n",
      " 28%|██▊       | 114348/414113 [00:25<01:05, 4568.22it/s]\u001b[A\n",
      " 28%|██▊       | 114818/414113 [00:25<01:04, 4605.44it/s]\u001b[A\n",
      " 28%|██▊       | 115284/414113 [00:25<01:04, 4621.30it/s]\u001b[A\n",
      " 28%|██▊       | 115747/414113 [00:25<01:04, 4600.98it/s]\u001b[A\n",
      " 28%|██▊       | 116218/414113 [00:25<01:04, 4632.10it/s]\u001b[A\n",
      " 28%|██▊       | 116683/414113 [00:26<01:04, 4634.27it/s]\u001b[A\n",
      " 28%|██▊       | 117147/414113 [00:26<01:04, 4619.20it/s]\u001b[A\n",
      " 28%|██▊       | 117625/414113 [00:26<01:03, 4663.64it/s]\u001b[A\n",
      " 29%|██▊       | 118092/414113 [00:26<01:03, 4652.71it/s]\u001b[A\n",
      " 29%|██▊       | 118558/414113 [00:26<01:04, 4616.64it/s]\u001b[A\n",
      " 29%|██▊       | 119033/414113 [00:26<01:03, 4653.66it/s]\u001b[A\n",
      " 29%|██▉       | 119507/414113 [00:26<01:02, 4677.57it/s]\u001b[A\n",
      " 29%|██▉       | 119975/414113 [00:26<01:02, 4670.02it/s]\u001b[A\n",
      " 29%|██▉       | 120443/414113 [00:26<01:03, 4635.28it/s]\u001b[A\n",
      " 29%|██▉       | 120907/414113 [00:26<01:03, 4597.26it/s]\u001b[A\n",
      " 29%|██▉       | 121381/414113 [00:27<01:03, 4638.62it/s]\u001b[A\n",
      " 29%|██▉       | 121848/414113 [00:27<01:02, 4647.99it/s]\u001b[A\n",
      " 30%|██▉       | 122313/414113 [00:27<01:02, 4640.96it/s]\u001b[A\n",
      " 30%|██▉       | 122778/414113 [00:27<01:02, 4642.21it/s]\u001b[A\n",
      " 30%|██▉       | 123243/414113 [00:27<01:02, 4644.05it/s]\u001b[A\n",
      " 30%|██▉       | 123723/414113 [00:27<01:01, 4688.33it/s]\u001b[A\n",
      " 30%|██▉       | 124202/414113 [00:27<01:01, 4716.54it/s]\u001b[A\n",
      " 30%|███       | 124674/414113 [00:27<01:01, 4679.66it/s]\u001b[A\n",
      " 30%|███       | 125143/414113 [00:27<01:01, 4668.72it/s]\u001b[A\n",
      " 30%|███       | 125612/414113 [00:27<01:01, 4672.99it/s]\u001b[A\n",
      " 30%|███       | 126080/414113 [00:28<01:01, 4671.61it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 126548/414113 [00:28<01:01, 4659.01it/s]\u001b[A\n",
      " 31%|███       | 127018/414113 [00:28<01:01, 4669.70it/s]\u001b[A\n",
      " 31%|███       | 127486/414113 [00:28<01:01, 4650.14it/s]\u001b[A\n",
      " 31%|███       | 127952/414113 [00:28<01:01, 4620.03it/s]\u001b[A\n",
      " 31%|███       | 128426/414113 [00:28<01:01, 4653.03it/s]\u001b[A\n",
      " 31%|███       | 128893/414113 [00:28<01:01, 4657.54it/s]\u001b[A\n",
      " 31%|███       | 129359/414113 [00:28<01:01, 4652.69it/s]\u001b[A\n",
      " 31%|███▏      | 129825/414113 [00:28<01:06, 4278.56it/s]\u001b[A\n",
      " 31%|███▏      | 130300/414113 [00:29<01:04, 4408.62it/s]\u001b[A\n",
      " 32%|███▏      | 130772/414113 [00:29<01:03, 4497.07it/s]\u001b[A\n",
      " 32%|███▏      | 131242/414113 [00:29<01:02, 4553.07it/s]\u001b[A\n",
      " 32%|███▏      | 131701/414113 [00:29<01:01, 4562.28it/s]\u001b[A\n",
      " 32%|███▏      | 132160/414113 [00:29<01:01, 4560.28it/s]\u001b[A\n",
      " 32%|███▏      | 132618/414113 [00:29<01:02, 4536.39it/s]\u001b[A\n",
      " 32%|███▏      | 133073/414113 [00:29<01:02, 4496.24it/s]\u001b[A\n",
      " 32%|███▏      | 133524/414113 [00:29<01:02, 4482.45it/s]\u001b[A\n",
      " 32%|███▏      | 133977/414113 [00:29<01:02, 4495.32it/s]\u001b[A\n",
      " 32%|███▏      | 134427/414113 [00:29<01:02, 4480.25it/s]\u001b[A\n",
      " 33%|███▎      | 134876/414113 [00:30<01:02, 4475.23it/s]\u001b[A\n",
      " 33%|███▎      | 135346/414113 [00:30<01:01, 4538.14it/s]\u001b[A\n",
      " 33%|███▎      | 135806/414113 [00:30<01:01, 4554.36it/s]\u001b[A\n",
      " 33%|███▎      | 136262/414113 [00:30<01:01, 4523.52it/s]\u001b[A\n",
      " 33%|███▎      | 136715/414113 [00:30<01:01, 4506.83it/s]\u001b[A\n",
      " 33%|███▎      | 137171/414113 [00:30<01:01, 4520.94it/s]\u001b[A\n",
      " 33%|███▎      | 137636/414113 [00:30<01:00, 4558.34it/s]\u001b[A\n",
      " 33%|███▎      | 138099/414113 [00:30<01:00, 4578.36it/s]\u001b[A\n",
      " 33%|███▎      | 138563/414113 [00:30<00:59, 4594.21it/s]\u001b[A\n",
      " 34%|███▎      | 139023/414113 [00:30<01:00, 4573.57it/s]\u001b[A\n",
      " 34%|███▎      | 139487/414113 [00:31<00:59, 4592.59it/s]\u001b[A\n",
      " 34%|███▍      | 139947/414113 [00:31<01:00, 4503.08it/s]\u001b[A\n",
      " 34%|███▍      | 140407/414113 [00:31<01:00, 4530.26it/s]\u001b[A\n",
      " 34%|███▍      | 140861/414113 [00:31<01:00, 4531.30it/s]\u001b[A\n",
      " 34%|███▍      | 141319/414113 [00:31<01:00, 4544.46it/s]\u001b[A\n",
      " 34%|███▍      | 141789/414113 [00:31<00:59, 4586.87it/s]\u001b[A\n",
      " 34%|███▍      | 142252/414113 [00:31<00:59, 4599.01it/s]\u001b[A\n",
      " 34%|███▍      | 142713/414113 [00:31<00:59, 4564.31it/s]\u001b[A\n",
      " 35%|███▍      | 143170/414113 [00:31<00:59, 4541.13it/s]\u001b[A\n",
      " 35%|███▍      | 143638/414113 [00:31<00:59, 4581.14it/s]\u001b[A\n",
      " 35%|███▍      | 144097/414113 [00:32<00:58, 4577.46it/s]\u001b[A\n",
      " 35%|███▍      | 144567/414113 [00:32<00:58, 4611.86it/s]\u001b[A\n",
      " 35%|███▌      | 145029/414113 [00:32<00:58, 4599.00it/s]\u001b[A\n",
      " 35%|███▌      | 145490/414113 [00:32<00:58, 4586.64it/s]\u001b[A\n",
      " 35%|███▌      | 145966/414113 [00:32<00:57, 4636.53it/s]\u001b[A\n",
      " 35%|███▌      | 146440/414113 [00:32<00:57, 4665.43it/s]\u001b[A\n",
      " 35%|███▌      | 146907/414113 [00:32<00:57, 4660.45it/s]\u001b[A\n",
      " 36%|███▌      | 147374/414113 [00:32<00:57, 4627.81it/s]\u001b[A\n",
      " 36%|███▌      | 147837/414113 [00:32<00:59, 4501.38it/s]\u001b[A\n",
      " 36%|███▌      | 148298/414113 [00:32<00:58, 4531.65it/s]\u001b[A\n",
      " 36%|███▌      | 148776/414113 [00:33<00:57, 4602.56it/s]\u001b[A\n",
      " 36%|███▌      | 149237/414113 [00:33<00:57, 4595.13it/s]\u001b[A\n",
      " 36%|███▌      | 149698/414113 [00:33<00:57, 4591.08it/s]\u001b[A\n",
      " 36%|███▋      | 150158/414113 [00:33<00:57, 4582.12it/s]\u001b[A\n",
      " 36%|███▋      | 150637/414113 [00:33<00:56, 4641.47it/s]\u001b[A\n",
      " 36%|███▋      | 151102/414113 [00:33<00:56, 4628.10it/s]\u001b[A\n",
      " 37%|███▋      | 151566/414113 [00:33<00:56, 4611.72it/s]\u001b[A\n",
      " 37%|███▋      | 152028/414113 [00:33<00:56, 4598.87it/s]\u001b[A\n",
      " 37%|███▋      | 152489/414113 [00:33<00:57, 4570.78it/s]\u001b[A\n",
      " 37%|███▋      | 152947/414113 [00:33<00:57, 4534.15it/s]\u001b[A\n",
      " 37%|███▋      | 153401/414113 [00:34<00:57, 4504.78it/s]\u001b[A\n",
      " 37%|███▋      | 153856/414113 [00:34<00:57, 4516.77it/s]\u001b[A\n",
      " 37%|███▋      | 154308/414113 [00:34<00:57, 4507.38it/s]\u001b[A\n",
      " 37%|███▋      | 154763/414113 [00:34<00:57, 4519.36it/s]\u001b[A\n",
      " 37%|███▋      | 155226/414113 [00:34<00:56, 4551.28it/s]\u001b[A\n",
      " 38%|███▊      | 155694/414113 [00:34<00:56, 4587.50it/s]\u001b[A\n",
      " 38%|███▊      | 156158/414113 [00:34<00:56, 4600.64it/s]\u001b[A\n",
      " 38%|███▊      | 156625/414113 [00:34<00:55, 4619.23it/s]\u001b[A\n",
      " 38%|███▊      | 157095/414113 [00:34<00:55, 4641.27it/s]\u001b[A\n",
      " 38%|███▊      | 157560/414113 [00:34<00:55, 4628.08it/s]\u001b[A\n",
      " 38%|███▊      | 158043/414113 [00:35<00:54, 4686.65it/s]\u001b[A\n",
      " 38%|███▊      | 158512/414113 [00:35<00:54, 4660.60it/s]\u001b[A\n",
      " 38%|███▊      | 158979/414113 [00:35<00:55, 4620.12it/s]\u001b[A\n",
      " 39%|███▊      | 159442/414113 [00:35<00:55, 4596.59it/s]\u001b[A\n",
      " 39%|███▊      | 159914/414113 [00:35<00:54, 4632.63it/s]\u001b[A\n",
      " 39%|███▊      | 160397/414113 [00:35<00:54, 4687.81it/s]\u001b[A\n",
      " 39%|███▉      | 160868/414113 [00:35<00:53, 4692.66it/s]\u001b[A\n",
      " 39%|███▉      | 161338/414113 [00:35<00:53, 4681.79it/s]\u001b[A\n",
      " 39%|███▉      | 161807/414113 [00:35<00:54, 4616.94it/s]\u001b[A\n",
      " 39%|███▉      | 162270/414113 [00:36<00:54, 4610.43it/s]\u001b[A\n",
      " 39%|███▉      | 162734/414113 [00:36<00:54, 4615.67it/s]\u001b[A\n",
      " 39%|███▉      | 163206/414113 [00:36<00:54, 4645.84it/s]\u001b[A\n",
      " 40%|███▉      | 163672/414113 [00:36<00:53, 4647.40it/s]\u001b[A\n",
      " 40%|███▉      | 164137/414113 [00:36<00:54, 4628.13it/s]\u001b[A\n",
      " 40%|███▉      | 164612/414113 [00:36<00:53, 4663.76it/s]\u001b[A\n",
      " 40%|███▉      | 165086/414113 [00:36<00:53, 4685.82it/s]\u001b[A\n",
      " 40%|███▉      | 165555/414113 [00:36<00:53, 4646.90it/s]\u001b[A\n",
      " 40%|████      | 166020/414113 [00:36<00:54, 4589.20it/s]\u001b[A\n",
      " 40%|████      | 166480/414113 [00:36<00:54, 4515.68it/s]\u001b[A\n",
      " 40%|████      | 166933/414113 [00:37<00:54, 4498.88it/s]\u001b[A\n",
      " 40%|████      | 167390/414113 [00:37<00:54, 4518.46it/s]\u001b[A\n",
      " 41%|████      | 167865/414113 [00:37<00:53, 4584.67it/s]\u001b[A\n",
      " 41%|████      | 168327/414113 [00:37<00:53, 4592.98it/s]\u001b[A\n",
      " 41%|████      | 168797/414113 [00:37<00:53, 4622.81it/s]\u001b[A\n",
      " 41%|████      | 169260/414113 [00:37<00:53, 4613.96it/s]\u001b[A\n",
      " 41%|████      | 169725/414113 [00:37<00:52, 4622.12it/s]\u001b[A\n",
      " 41%|████      | 170188/414113 [00:37<00:53, 4572.81it/s]\u001b[A\n",
      " 41%|████      | 170652/414113 [00:37<00:53, 4591.51it/s]\u001b[A\n",
      " 41%|████▏     | 171112/414113 [00:37<00:53, 4556.66it/s]\u001b[A\n",
      " 41%|████▏     | 171574/414113 [00:38<00:53, 4573.67it/s]\u001b[A\n",
      " 42%|████▏     | 172050/414113 [00:38<00:52, 4627.05it/s]\u001b[A\n",
      " 42%|████▏     | 172513/414113 [00:38<00:52, 4587.69it/s]\u001b[A\n",
      " 42%|████▏     | 172973/414113 [00:38<00:52, 4562.45it/s]\u001b[A\n",
      " 42%|████▏     | 173430/414113 [00:38<00:52, 4553.97it/s]\u001b[A\n",
      " 42%|████▏     | 173886/414113 [00:38<00:52, 4537.33it/s]\u001b[A\n",
      " 42%|████▏     | 174355/414113 [00:38<00:52, 4579.73it/s]\u001b[A\n",
      " 42%|████▏     | 174814/414113 [00:38<00:52, 4573.93it/s]\u001b[A\n",
      " 42%|████▏     | 175272/414113 [00:38<00:54, 4406.77it/s]\u001b[A\n",
      " 42%|████▏     | 175728/414113 [00:38<00:53, 4451.02it/s]\u001b[A\n",
      " 43%|████▎     | 176177/414113 [00:39<00:53, 4461.80it/s]\u001b[A\n",
      " 43%|████▎     | 176654/414113 [00:39<00:52, 4549.73it/s]\u001b[A\n",
      " 43%|████▎     | 177120/414113 [00:39<00:51, 4579.83it/s]\u001b[A\n",
      " 43%|████▎     | 177584/414113 [00:39<00:51, 4596.97it/s]\u001b[A\n",
      " 43%|████▎     | 178045/414113 [00:39<00:51, 4599.40it/s]\u001b[A\n",
      " 43%|████▎     | 178506/414113 [00:39<00:51, 4570.19it/s]\u001b[A\n",
      " 43%|████▎     | 178973/414113 [00:39<00:51, 4599.01it/s]\u001b[A\n",
      " 43%|████▎     | 179434/414113 [00:39<00:51, 4599.48it/s]\u001b[A\n",
      " 43%|████▎     | 179895/414113 [00:39<00:51, 4528.89it/s]\u001b[A\n",
      " 44%|████▎     | 180349/414113 [00:39<00:52, 4474.88it/s]\u001b[A\n",
      " 44%|████▎     | 180816/414113 [00:40<00:51, 4531.00it/s]\u001b[A\n",
      " 44%|████▍     | 181283/414113 [00:40<00:50, 4570.93it/s]\u001b[A\n",
      " 44%|████▍     | 181743/414113 [00:40<00:50, 4576.99it/s]\u001b[A\n",
      " 44%|████▍     | 182201/414113 [00:40<00:54, 4271.27it/s]\u001b[A\n",
      " 44%|████▍     | 182659/414113 [00:40<00:53, 4356.52it/s]\u001b[A\n",
      " 44%|████▍     | 183110/414113 [00:40<00:52, 4400.54it/s]\u001b[A\n",
      " 44%|████▍     | 183571/414113 [00:40<00:51, 4459.61it/s]\u001b[A\n",
      " 44%|████▍     | 184025/414113 [00:40<00:51, 4481.38it/s]\u001b[A\n",
      " 45%|████▍     | 184476/414113 [00:40<00:51, 4488.74it/s]\u001b[A\n",
      " 45%|████▍     | 184929/414113 [00:40<00:50, 4499.85it/s]\u001b[A\n",
      " 45%|████▍     | 185408/414113 [00:41<00:49, 4581.67it/s]\u001b[A\n",
      " 45%|████▍     | 185877/414113 [00:41<00:49, 4612.37it/s]\u001b[A\n",
      " 45%|████▍     | 186339/414113 [00:41<00:49, 4613.04it/s]\u001b[A\n",
      " 45%|████▌     | 186806/414113 [00:41<00:49, 4629.47it/s]\u001b[A\n",
      " 45%|████▌     | 187271/414113 [00:41<00:48, 4633.33it/s]\u001b[A\n",
      " 45%|████▌     | 187740/414113 [00:41<00:48, 4649.76it/s]\u001b[A\n",
      " 45%|████▌     | 188206/414113 [00:41<00:48, 4643.11it/s]\u001b[A\n",
      " 46%|████▌     | 188671/414113 [00:41<00:49, 4508.98it/s]\u001b[A\n",
      " 46%|████▌     | 189127/414113 [00:41<00:49, 4523.32it/s]\u001b[A\n",
      " 46%|████▌     | 189580/414113 [00:42<00:49, 4504.18it/s]\u001b[A\n",
      " 46%|████▌     | 190044/414113 [00:42<00:49, 4543.91it/s]\u001b[A\n",
      " 46%|████▌     | 190515/414113 [00:42<00:48, 4590.15it/s]\u001b[A\n",
      " 46%|████▌     | 190981/414113 [00:42<00:48, 4609.70it/s]\u001b[A\n",
      " 46%|████▌     | 191458/414113 [00:42<00:47, 4654.40it/s]\u001b[A\n",
      " 46%|████▋     | 191932/414113 [00:42<00:47, 4677.67it/s]\u001b[A\n",
      " 46%|████▋     | 192401/414113 [00:42<00:47, 4653.64it/s]\u001b[A\n",
      " 47%|████▋     | 192867/414113 [00:42<00:47, 4629.85it/s]\u001b[A\n",
      " 47%|████▋     | 193331/414113 [00:42<00:48, 4567.52it/s]\u001b[A\n",
      " 47%|████▋     | 193791/414113 [00:42<00:48, 4576.10it/s]\u001b[A\n",
      " 47%|████▋     | 194249/414113 [00:43<00:48, 4570.95it/s]\u001b[A\n",
      " 47%|████▋     | 194711/414113 [00:43<00:47, 4584.10it/s]\u001b[A\n",
      " 47%|████▋     | 195181/414113 [00:43<00:47, 4615.92it/s]\u001b[A\n",
      " 47%|████▋     | 195649/414113 [00:43<00:47, 4634.24it/s]\u001b[A\n",
      " 47%|████▋     | 196113/414113 [00:43<01:29, 2439.83it/s]\u001b[A\n",
      " 47%|████▋     | 196558/414113 [00:43<01:17, 2821.74it/s]\u001b[A\n",
      " 48%|████▊     | 197011/414113 [00:43<01:08, 3181.66it/s]\u001b[A\n",
      " 48%|████▊     | 197458/414113 [00:44<01:02, 3482.73it/s]\u001b[A\n",
      " 48%|████▊     | 197905/414113 [00:44<00:57, 3728.51it/s]\u001b[A\n",
      " 48%|████▊     | 198366/414113 [00:44<00:54, 3954.69it/s]\u001b[A\n",
      " 48%|████▊     | 198814/414113 [00:44<00:52, 4098.80it/s]\u001b[A\n",
      " 48%|████▊     | 199268/414113 [00:44<00:50, 4221.76it/s]\u001b[A\n",
      " 48%|████▊     | 199736/414113 [00:44<00:49, 4347.32it/s]\u001b[A\n",
      " 48%|████▊     | 200203/414113 [00:44<00:48, 4439.23it/s]\u001b[A\n",
      " 48%|████▊     | 200666/414113 [00:44<00:47, 4492.45it/s]\u001b[A\n",
      " 49%|████▊     | 201124/414113 [00:44<00:47, 4507.56it/s]\u001b[A\n",
      " 49%|████▊     | 201581/414113 [00:44<00:47, 4513.79it/s]\u001b[A\n",
      " 49%|████▉     | 202048/414113 [00:45<00:46, 4557.49it/s]\u001b[A\n",
      " 49%|████▉     | 202529/414113 [00:45<00:45, 4628.98it/s]\u001b[A\n",
      " 49%|████▉     | 203000/414113 [00:45<00:45, 4652.58it/s]\u001b[A\n",
      " 49%|████▉     | 203467/414113 [00:45<00:45, 4601.21it/s]\u001b[A\n",
      " 49%|████▉     | 203929/414113 [00:45<00:46, 4557.86it/s]\u001b[A\n",
      " 49%|████▉     | 204387/414113 [00:45<00:45, 4562.85it/s]\u001b[A\n",
      " 49%|████▉     | 204854/414113 [00:45<00:45, 4592.96it/s]\u001b[A\n",
      " 50%|████▉     | 205314/414113 [00:45<00:46, 4535.64it/s]\u001b[A\n",
      " 50%|████▉     | 205769/414113 [00:45<00:46, 4474.30it/s]\u001b[A\n",
      " 50%|████▉     | 206217/414113 [00:45<00:46, 4451.32it/s]\u001b[A\n",
      " 50%|████▉     | 206689/414113 [00:46<00:45, 4527.94it/s]\u001b[A\n",
      " 50%|█████     | 207143/414113 [00:46<00:45, 4528.42it/s]\u001b[A\n",
      " 50%|█████     | 207597/414113 [00:46<00:45, 4518.45it/s]\u001b[A\n",
      " 50%|█████     | 208053/414113 [00:46<00:45, 4528.57it/s]\u001b[A\n",
      " 50%|█████     | 208507/414113 [00:46<00:45, 4499.74it/s]\u001b[A\n",
      " 50%|█████     | 208958/414113 [00:46<00:46, 4372.78it/s]\u001b[A\n",
      " 51%|█████     | 209430/414113 [00:46<00:45, 4469.47it/s]\u001b[A\n",
      " 51%|█████     | 209880/414113 [00:46<00:45, 4476.75it/s]\u001b[A\n",
      " 51%|█████     | 210329/414113 [00:46<00:45, 4463.19it/s]\u001b[A\n",
      " 51%|█████     | 210787/414113 [00:46<00:45, 4495.26it/s]\u001b[A\n",
      " 51%|█████     | 211237/414113 [00:47<00:45, 4456.29it/s]\u001b[A\n",
      " 51%|█████     | 211690/414113 [00:47<00:45, 4477.63it/s]\u001b[A\n",
      " 51%|█████     | 212147/414113 [00:47<00:44, 4502.55it/s]\u001b[A\n",
      " 51%|█████▏    | 212598/414113 [00:47<00:45, 4437.26it/s]\u001b[A\n",
      " 51%|█████▏    | 213047/414113 [00:47<00:45, 4452.31it/s]\u001b[A\n",
      " 52%|█████▏    | 213498/414113 [00:47<00:44, 4468.97it/s]\u001b[A\n",
      " 52%|█████▏    | 213946/414113 [00:47<00:45, 4445.00it/s]\u001b[A\n",
      " 52%|█████▏    | 214404/414113 [00:47<00:44, 4482.87it/s]\u001b[A\n",
      " 52%|█████▏    | 214853/414113 [00:47<00:44, 4443.37it/s]\u001b[A\n",
      " 52%|█████▏    | 215308/414113 [00:47<00:44, 4472.88it/s]\u001b[A\n",
      " 52%|█████▏    | 215759/414113 [00:48<00:44, 4483.32it/s]\u001b[A\n",
      " 52%|█████▏    | 216231/414113 [00:48<00:43, 4550.31it/s]\u001b[A\n",
      " 52%|█████▏    | 216691/414113 [00:48<00:43, 4563.91it/s]\u001b[A\n",
      " 52%|█████▏    | 217148/414113 [00:48<00:43, 4546.37it/s]\u001b[A\n",
      " 53%|█████▎    | 217603/414113 [00:48<00:44, 4464.44it/s]\u001b[A\n",
      " 53%|█████▎    | 218054/414113 [00:48<00:43, 4476.31it/s]\u001b[A\n",
      " 53%|█████▎    | 218529/414113 [00:48<00:42, 4553.94it/s]\u001b[A\n",
      " 53%|█████▎    | 218985/414113 [00:48<00:42, 4552.23it/s]\u001b[A\n",
      " 53%|█████▎    | 219445/414113 [00:48<00:42, 4566.43it/s]\u001b[A\n",
      " 53%|█████▎    | 219908/414113 [00:48<00:42, 4584.36it/s]\u001b[A\n",
      " 53%|█████▎    | 220382/414113 [00:49<00:41, 4629.78it/s]\u001b[A\n",
      " 53%|█████▎    | 220848/414113 [00:49<00:41, 4637.34it/s]\u001b[A\n",
      " 53%|█████▎    | 221312/414113 [00:49<00:41, 4617.46it/s]\u001b[A\n",
      " 54%|█████▎    | 221774/414113 [00:49<00:41, 4602.67it/s]\u001b[A\n",
      " 54%|█████▎    | 222242/414113 [00:49<00:41, 4623.37it/s]\u001b[A\n",
      " 54%|█████▍    | 222705/414113 [00:49<00:41, 4614.29it/s]\u001b[A\n",
      " 54%|█████▍    | 223167/414113 [00:49<00:41, 4586.80it/s]\u001b[A\n",
      " 54%|█████▍    | 223626/414113 [00:49<00:41, 4555.46it/s]\u001b[A\n",
      " 54%|█████▍    | 224082/414113 [00:49<00:41, 4551.56it/s]\u001b[A\n",
      " 54%|█████▍    | 224538/414113 [00:49<00:41, 4528.64it/s]\u001b[A\n",
      " 54%|█████▍    | 225012/414113 [00:50<00:41, 4589.65it/s]\u001b[A\n",
      " 54%|█████▍    | 225474/414113 [00:50<00:41, 4596.93it/s]\u001b[A\n",
      " 55%|█████▍    | 225934/414113 [00:50<00:41, 4586.09it/s]\u001b[A\n",
      " 55%|█████▍    | 226393/414113 [00:50<00:40, 4584.52it/s]\u001b[A\n",
      " 55%|█████▍    | 226852/414113 [00:50<00:41, 4503.59it/s]\u001b[A\n",
      " 55%|█████▍    | 227320/414113 [00:50<00:41, 4553.18it/s]\u001b[A\n",
      " 55%|█████▌    | 227796/414113 [00:50<00:40, 4612.79it/s]\u001b[A\n",
      " 55%|█████▌    | 228258/414113 [00:50<00:40, 4592.56it/s]\u001b[A\n",
      " 55%|█████▌    | 228718/414113 [00:50<00:40, 4573.37it/s]\u001b[A\n",
      " 55%|█████▌    | 229176/414113 [00:50<00:40, 4544.12it/s]\u001b[A\n",
      " 55%|█████▌    | 229631/414113 [00:51<00:40, 4519.58it/s]\u001b[A\n",
      " 56%|█████▌    | 230090/414113 [00:51<00:40, 4538.66it/s]\u001b[A\n",
      " 56%|█████▌    | 230545/414113 [00:51<00:40, 4501.57it/s]\u001b[A\n",
      " 56%|█████▌    | 230999/414113 [00:51<00:40, 4510.09it/s]\u001b[A\n",
      " 56%|█████▌    | 231451/414113 [00:51<00:40, 4499.91it/s]\u001b[A\n",
      " 56%|█████▌    | 231912/414113 [00:51<00:40, 4531.27it/s]\u001b[A\n",
      " 56%|█████▌    | 232366/414113 [00:51<00:40, 4504.44it/s]\u001b[A\n",
      " 56%|█████▌    | 232817/414113 [00:51<00:40, 4461.76it/s]\u001b[A\n",
      " 56%|█████▋    | 233266/414113 [00:51<00:40, 4469.14it/s]\u001b[A\n",
      " 56%|█████▋    | 233719/414113 [00:52<00:40, 4485.55it/s]\u001b[A\n",
      " 57%|█████▋    | 234184/414113 [00:52<00:39, 4531.23it/s]\u001b[A\n",
      " 57%|█████▋    | 234644/414113 [00:52<00:39, 4550.46it/s]\u001b[A\n",
      " 57%|█████▋    | 235100/414113 [00:52<00:39, 4531.73it/s]\u001b[A\n",
      " 57%|█████▋    | 235554/414113 [00:52<00:39, 4521.99it/s]\u001b[A\n",
      " 57%|█████▋    | 236020/414113 [00:52<00:39, 4561.38it/s]\u001b[A\n",
      " 57%|█████▋    | 236477/414113 [00:52<00:38, 4558.56it/s]\u001b[A\n",
      " 57%|█████▋    | 236933/414113 [00:52<00:39, 4524.87it/s]\u001b[A\n",
      " 57%|█████▋    | 237386/414113 [00:52<00:39, 4469.71it/s]\u001b[A\n",
      " 57%|█████▋    | 237834/414113 [00:52<00:39, 4462.73it/s]\u001b[A\n",
      " 58%|█████▊    | 238288/414113 [00:53<00:39, 4482.82it/s]\u001b[A\n",
      " 58%|█████▊    | 238747/414113 [00:53<00:38, 4512.74it/s]\u001b[A\n",
      " 58%|█████▊    | 239199/414113 [00:53<00:39, 4469.49it/s]\u001b[A\n",
      " 58%|█████▊    | 239647/414113 [00:53<00:39, 4417.23it/s]\u001b[A\n",
      " 58%|█████▊    | 240090/414113 [00:53<00:39, 4400.90it/s]\u001b[A\n",
      " 58%|█████▊    | 240539/414113 [00:53<00:39, 4425.69it/s]\u001b[A\n",
      " 58%|█████▊    | 241002/414113 [00:53<00:38, 4484.39it/s]\u001b[A\n",
      " 58%|█████▊    | 241451/414113 [00:53<00:38, 4471.00it/s]\u001b[A\n",
      " 58%|█████▊    | 241899/414113 [00:53<00:38, 4428.30it/s]\u001b[A\n",
      " 59%|█████▊    | 242343/414113 [00:53<00:38, 4430.07it/s]\u001b[A\n",
      " 59%|█████▊    | 242787/414113 [00:54<00:38, 4430.19it/s]\u001b[A\n",
      " 59%|█████▊    | 243231/414113 [00:54<00:38, 4388.78it/s]\u001b[A\n",
      " 59%|█████▉    | 243683/414113 [00:54<00:38, 4425.50it/s]\u001b[A\n",
      " 59%|█████▉    | 244129/414113 [00:54<00:38, 4433.44it/s]\u001b[A\n",
      " 59%|█████▉    | 244577/414113 [00:54<00:38, 4446.07it/s]\u001b[A\n",
      " 59%|█████▉    | 245036/414113 [00:54<00:37, 4487.72it/s]\u001b[A\n",
      " 59%|█████▉    | 245491/414113 [00:54<00:37, 4504.60it/s]\u001b[A\n",
      " 59%|█████▉    | 245942/414113 [00:54<00:37, 4449.74it/s]\u001b[A\n",
      " 59%|█████▉    | 246388/414113 [00:54<00:37, 4447.09it/s]\u001b[A\n",
      " 60%|█████▉    | 246835/414113 [00:54<00:37, 4451.75it/s]\u001b[A\n",
      " 60%|█████▉    | 247281/414113 [00:55<00:37, 4442.96it/s]\u001b[A\n",
      " 60%|█████▉    | 247738/414113 [00:55<00:37, 4479.64it/s]\u001b[A\n",
      " 60%|█████▉    | 248187/414113 [00:55<00:37, 4460.94it/s]\u001b[A\n",
      " 60%|██████    | 248634/414113 [00:55<00:37, 4429.81it/s]\u001b[A\n",
      " 60%|██████    | 249078/414113 [00:55<00:37, 4426.64it/s]\u001b[A\n",
      " 60%|██████    | 249522/414113 [00:55<00:37, 4429.97it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 249977/414113 [00:55<00:36, 4465.23it/s]\u001b[A\n",
      " 60%|██████    | 250432/414113 [00:55<00:36, 4489.20it/s]\u001b[A\n",
      " 61%|██████    | 250882/414113 [00:55<00:36, 4437.96it/s]\u001b[A\n",
      " 61%|██████    | 251334/414113 [00:55<00:36, 4461.62it/s]\u001b[A\n",
      " 61%|██████    | 251805/414113 [00:56<00:35, 4531.37it/s]\u001b[A\n",
      " 61%|██████    | 252277/414113 [00:56<00:35, 4586.10it/s]\u001b[A\n",
      " 61%|██████    | 252737/414113 [00:56<00:35, 4543.85it/s]\u001b[A\n",
      " 61%|██████    | 253193/414113 [00:56<00:35, 4547.08it/s]\u001b[A\n",
      " 61%|██████▏   | 253648/414113 [00:56<00:35, 4531.95it/s]\u001b[A\n",
      " 61%|██████▏   | 254102/414113 [00:56<00:35, 4520.85it/s]\u001b[A\n",
      " 61%|██████▏   | 254562/414113 [00:56<00:35, 4541.43it/s]\u001b[A\n",
      " 62%|██████▏   | 255023/414113 [00:56<00:34, 4559.53it/s]\u001b[A\n",
      " 62%|██████▏   | 255480/414113 [00:56<00:35, 4506.90it/s]\u001b[A\n",
      " 62%|██████▏   | 255931/414113 [00:56<00:35, 4477.83it/s]\u001b[A\n",
      " 62%|██████▏   | 256390/414113 [00:57<00:34, 4508.28it/s]\u001b[A\n",
      " 62%|██████▏   | 256844/414113 [00:57<00:34, 4515.82it/s]\u001b[A\n",
      " 62%|██████▏   | 257299/414113 [00:57<00:34, 4524.96it/s]\u001b[A\n",
      " 62%|██████▏   | 257762/414113 [00:57<00:34, 4554.28it/s]\u001b[A\n",
      " 62%|██████▏   | 258236/414113 [00:57<00:33, 4607.18it/s]\u001b[A\n",
      " 62%|██████▏   | 258697/414113 [00:57<00:34, 4561.48it/s]\u001b[A\n",
      " 63%|██████▎   | 259154/414113 [00:57<00:34, 4541.90it/s]\u001b[A\n",
      " 63%|██████▎   | 259611/414113 [00:57<00:33, 4547.76it/s]\u001b[A\n",
      " 63%|██████▎   | 260066/414113 [00:57<00:34, 4520.72it/s]\u001b[A\n",
      " 63%|██████▎   | 260519/414113 [00:58<00:36, 4158.32it/s]\u001b[A\n",
      " 63%|██████▎   | 261003/414113 [00:58<00:35, 4340.53it/s]\u001b[A\n",
      " 63%|██████▎   | 261473/414113 [00:58<00:34, 4441.88it/s]\u001b[A\n",
      " 63%|██████▎   | 261932/414113 [00:58<00:33, 4483.62it/s]\u001b[A\n",
      " 63%|██████▎   | 262384/414113 [00:58<00:34, 4454.01it/s]\u001b[A\n",
      " 63%|██████▎   | 262833/414113 [00:58<00:33, 4463.51it/s]\u001b[A\n",
      " 64%|██████▎   | 263282/414113 [00:58<00:33, 4440.77it/s]\u001b[A\n",
      " 64%|██████▎   | 263728/414113 [00:58<00:33, 4434.65it/s]\u001b[A\n",
      " 64%|██████▍   | 264178/414113 [00:58<00:33, 4451.01it/s]\u001b[A\n",
      " 64%|██████▍   | 264624/414113 [00:58<00:33, 4422.69it/s]\u001b[A\n",
      " 64%|██████▍   | 265067/414113 [00:59<00:33, 4418.98it/s]\u001b[A\n",
      " 64%|██████▍   | 265532/414113 [00:59<00:33, 4484.02it/s]\u001b[A\n",
      " 64%|██████▍   | 265995/414113 [00:59<00:32, 4525.10it/s]\u001b[A\n",
      " 64%|██████▍   | 266454/414113 [00:59<00:32, 4542.86it/s]\u001b[A\n",
      " 64%|██████▍   | 266913/414113 [00:59<00:32, 4555.39it/s]\u001b[A\n",
      " 65%|██████▍   | 267381/414113 [00:59<00:31, 4589.64it/s]\u001b[A\n",
      " 65%|██████▍   | 267841/414113 [00:59<00:32, 4530.57it/s]\u001b[A\n",
      " 65%|██████▍   | 268295/414113 [00:59<00:32, 4527.38it/s]\u001b[A\n",
      " 65%|██████▍   | 268748/414113 [00:59<00:32, 4516.21it/s]\u001b[A\n",
      " 65%|██████▌   | 269200/414113 [00:59<00:32, 4496.02it/s]\u001b[A\n",
      " 65%|██████▌   | 269650/414113 [01:00<00:32, 4480.84it/s]\u001b[A\n",
      " 65%|██████▌   | 270111/414113 [01:00<00:31, 4518.10it/s]\u001b[A\n",
      " 65%|██████▌   | 270563/414113 [01:00<00:31, 4509.55it/s]\u001b[A\n",
      " 65%|██████▌   | 271015/414113 [01:00<00:31, 4476.29it/s]\u001b[A\n",
      " 66%|██████▌   | 271463/414113 [01:00<00:31, 4469.39it/s]\u001b[A\n",
      " 66%|██████▌   | 271911/414113 [01:00<00:32, 4440.24it/s]\u001b[A\n",
      " 66%|██████▌   | 272360/414113 [01:00<00:31, 4453.41it/s]\u001b[A\n",
      " 66%|██████▌   | 272811/414113 [01:00<00:31, 4469.70it/s]\u001b[A\n",
      " 66%|██████▌   | 273259/414113 [01:00<00:31, 4468.96it/s]\u001b[A\n",
      " 66%|██████▌   | 273706/414113 [01:00<00:31, 4459.24it/s]\u001b[A\n",
      " 66%|██████▌   | 274159/414113 [01:01<00:31, 4478.39it/s]\u001b[A\n",
      " 66%|██████▋   | 274624/414113 [01:01<00:30, 4528.18it/s]\u001b[A\n",
      " 66%|██████▋   | 275077/414113 [01:01<00:30, 4516.98it/s]\u001b[A\n",
      " 67%|██████▋   | 275531/414113 [01:01<00:30, 4523.02it/s]\u001b[A\n",
      " 67%|██████▋   | 275986/414113 [01:01<00:30, 4529.95it/s]\u001b[A\n",
      " 67%|██████▋   | 276440/414113 [01:01<00:30, 4503.57it/s]\u001b[A\n",
      " 67%|██████▋   | 276891/414113 [01:01<00:30, 4504.40it/s]\u001b[A\n",
      " 67%|██████▋   | 277342/414113 [01:01<00:30, 4467.01it/s]\u001b[A\n",
      " 67%|██████▋   | 277789/414113 [01:01<00:30, 4441.01it/s]\u001b[A\n",
      " 67%|██████▋   | 278253/414113 [01:01<00:30, 4496.18it/s]\u001b[A\n",
      " 67%|██████▋   | 278706/414113 [01:02<00:30, 4506.07it/s]\u001b[A\n",
      " 67%|██████▋   | 279172/414113 [01:02<00:29, 4550.32it/s]\u001b[A\n",
      " 68%|██████▊   | 279628/414113 [01:02<00:29, 4515.50it/s]\u001b[A\n",
      " 68%|██████▊   | 280085/414113 [01:02<00:29, 4530.66it/s]\u001b[A\n",
      " 68%|██████▊   | 280548/414113 [01:02<00:29, 4559.83it/s]\u001b[A\n",
      " 68%|██████▊   | 281005/414113 [01:02<00:29, 4535.17it/s]\u001b[A\n",
      " 68%|██████▊   | 281465/414113 [01:02<00:29, 4553.22it/s]\u001b[A\n",
      " 68%|██████▊   | 281921/414113 [01:02<00:29, 4553.12it/s]\u001b[A\n",
      " 68%|██████▊   | 282377/414113 [01:02<00:30, 4308.86it/s]\u001b[A\n",
      " 68%|██████▊   | 282828/414113 [01:02<00:30, 4364.98it/s]\u001b[A\n",
      " 68%|██████▊   | 283286/414113 [01:03<00:29, 4426.20it/s]\u001b[A\n",
      " 69%|██████▊   | 283764/414113 [01:03<00:28, 4525.10it/s]\u001b[A\n",
      " 69%|██████▊   | 284219/414113 [01:03<00:29, 4452.52it/s]\u001b[A\n",
      " 69%|██████▊   | 284668/414113 [01:03<00:29, 4461.98it/s]\u001b[A\n",
      " 69%|██████▉   | 285116/414113 [01:03<00:28, 4452.85it/s]\u001b[A\n",
      " 69%|██████▉   | 285563/414113 [01:03<00:28, 4447.16it/s]\u001b[A\n",
      " 69%|██████▉   | 286026/414113 [01:03<00:28, 4499.51it/s]\u001b[A\n",
      " 69%|██████▉   | 286477/414113 [01:03<00:28, 4486.95it/s]\u001b[A\n",
      " 69%|██████▉   | 286929/414113 [01:03<00:28, 4496.76it/s]\u001b[A\n",
      " 69%|██████▉   | 287379/414113 [01:03<00:28, 4443.18it/s]\u001b[A\n",
      " 70%|██████▉   | 287834/414113 [01:04<00:28, 4472.80it/s]\u001b[A\n",
      " 70%|██████▉   | 288286/414113 [01:04<00:28, 4486.38it/s]\u001b[A\n",
      " 70%|██████▉   | 288738/414113 [01:04<00:27, 4494.74it/s]\u001b[A\n",
      " 70%|██████▉   | 289193/414113 [01:04<00:27, 4509.53it/s]\u001b[A\n",
      " 70%|██████▉   | 289652/414113 [01:04<00:27, 4532.59it/s]\u001b[A\n",
      " 70%|███████   | 290106/414113 [01:04<00:27, 4473.08it/s]\u001b[A\n",
      " 70%|███████   | 290577/414113 [01:04<00:27, 4539.42it/s]\u001b[A\n",
      " 70%|███████   | 291032/414113 [01:04<00:27, 4490.96it/s]\u001b[A\n",
      " 70%|███████   | 291482/414113 [01:04<00:27, 4448.46it/s]\u001b[A\n",
      " 70%|███████   | 291939/414113 [01:04<00:27, 4482.12it/s]\u001b[A\n",
      " 71%|███████   | 292402/414113 [01:05<00:26, 4522.72it/s]\u001b[A\n",
      " 71%|███████   | 292856/414113 [01:05<00:26, 4526.98it/s]\u001b[A\n",
      " 71%|███████   | 293320/414113 [01:05<00:26, 4558.55it/s]\u001b[A\n",
      " 71%|███████   | 293777/414113 [01:05<00:26, 4558.92it/s]\u001b[A\n",
      " 71%|███████   | 294234/414113 [01:05<00:26, 4547.99it/s]\u001b[A\n",
      " 71%|███████   | 294689/414113 [01:05<00:26, 4521.79it/s]\u001b[A\n",
      " 71%|███████▏  | 295142/414113 [01:05<00:26, 4512.54it/s]\u001b[A\n",
      " 71%|███████▏  | 295594/414113 [01:05<00:26, 4504.88it/s]\u001b[A\n",
      " 71%|███████▏  | 296045/414113 [01:05<00:26, 4468.70it/s]\u001b[A\n",
      " 72%|███████▏  | 296492/414113 [01:05<00:26, 4467.96it/s]\u001b[A\n",
      " 72%|███████▏  | 296942/414113 [01:06<00:26, 4476.80it/s]\u001b[A\n",
      " 72%|███████▏  | 297390/414113 [01:06<00:26, 4468.09it/s]\u001b[A\n",
      " 72%|███████▏  | 297837/414113 [01:06<00:26, 4407.72it/s]\u001b[A\n",
      " 72%|███████▏  | 298288/414113 [01:06<00:26, 4435.49it/s]\u001b[A\n",
      " 72%|███████▏  | 298740/414113 [01:06<00:25, 4459.64it/s]\u001b[A\n",
      " 72%|███████▏  | 299193/414113 [01:06<00:25, 4479.43it/s]\u001b[A\n",
      " 72%|███████▏  | 299652/414113 [01:06<00:25, 4511.35it/s]\u001b[A\n",
      " 72%|███████▏  | 300104/414113 [01:06<00:25, 4509.56it/s]\u001b[A\n",
      " 73%|███████▎  | 300556/414113 [01:06<00:25, 4464.57it/s]\u001b[A\n",
      " 73%|███████▎  | 301009/414113 [01:07<00:25, 4482.78it/s]\u001b[A\n",
      " 73%|███████▎  | 301458/414113 [01:07<00:25, 4478.71it/s]\u001b[A\n",
      " 73%|███████▎  | 301906/414113 [01:07<00:25, 4389.50it/s]\u001b[A\n",
      " 73%|███████▎  | 302357/414113 [01:07<00:25, 4423.48it/s]\u001b[A\n",
      " 73%|███████▎  | 302800/414113 [01:07<00:25, 4417.93it/s]\u001b[A\n",
      " 73%|███████▎  | 303248/414113 [01:07<00:24, 4435.90it/s]\u001b[A\n",
      " 73%|███████▎  | 303704/414113 [01:07<00:24, 4470.67it/s]\u001b[A\n",
      " 73%|███████▎  | 304152/414113 [01:07<00:24, 4467.34it/s]\u001b[A\n",
      " 74%|███████▎  | 304599/414113 [01:07<00:24, 4456.20it/s]\u001b[A\n",
      " 74%|███████▎  | 305045/414113 [01:07<00:24, 4396.14it/s]\u001b[A\n",
      " 74%|███████▍  | 305499/414113 [01:08<00:24, 4437.28it/s]\u001b[A\n",
      " 74%|███████▍  | 305956/414113 [01:08<00:24, 4473.95it/s]\u001b[A\n",
      " 74%|███████▍  | 306409/414113 [01:08<00:23, 4490.37it/s]\u001b[A\n",
      " 74%|███████▍  | 306859/414113 [01:08<00:23, 4481.08it/s]\u001b[A\n",
      " 74%|███████▍  | 307308/414113 [01:08<00:23, 4466.28it/s]\u001b[A\n",
      " 74%|███████▍  | 307759/414113 [01:08<00:23, 4479.27it/s]\u001b[A\n",
      " 74%|███████▍  | 308221/414113 [01:08<00:23, 4519.90it/s]\u001b[A\n",
      " 75%|███████▍  | 308674/414113 [01:08<00:23, 4487.92it/s]\u001b[A\n",
      " 75%|███████▍  | 309123/414113 [01:08<00:23, 4448.11it/s]\u001b[A\n",
      " 75%|███████▍  | 309569/414113 [01:08<00:23, 4441.83it/s]\u001b[A\n",
      " 75%|███████▍  | 310034/414113 [01:09<00:23, 4501.66it/s]\u001b[A\n",
      " 75%|███████▍  | 310485/414113 [01:09<00:23, 4412.22it/s]\u001b[A\n",
      " 75%|███████▌  | 310937/414113 [01:09<00:23, 4442.68it/s]\u001b[A\n",
      " 75%|███████▌  | 311392/414113 [01:09<00:22, 4471.63it/s]\u001b[A\n",
      " 75%|███████▌  | 311851/414113 [01:09<00:22, 4506.09it/s]\u001b[A\n",
      " 75%|███████▌  | 312311/414113 [01:09<00:22, 4531.08it/s]\u001b[A\n",
      " 76%|███████▌  | 312774/414113 [01:09<00:22, 4558.93it/s]\u001b[A\n",
      " 76%|███████▌  | 313231/414113 [01:09<00:22, 4556.02it/s]\u001b[A\n",
      " 76%|███████▌  | 313687/414113 [01:09<00:22, 4515.44it/s]\u001b[A\n",
      " 76%|███████▌  | 314139/414113 [01:09<00:22, 4492.78it/s]\u001b[A\n",
      " 76%|███████▌  | 314591/414113 [01:10<00:22, 4498.97it/s]\u001b[A\n",
      " 76%|███████▌  | 315058/414113 [01:10<00:21, 4546.42it/s]\u001b[A\n",
      " 76%|███████▌  | 315525/414113 [01:10<00:21, 4581.27it/s]\u001b[A\n",
      " 76%|███████▋  | 315996/414113 [01:10<00:21, 4618.84it/s]\u001b[A\n",
      " 76%|███████▋  | 316459/414113 [01:10<00:21, 4618.98it/s]\u001b[A\n",
      " 77%|███████▋  | 316926/414113 [01:10<00:20, 4631.88it/s]\u001b[A\n",
      " 77%|███████▋  | 317390/414113 [01:10<00:20, 4626.76it/s]\u001b[A\n",
      " 77%|███████▋  | 317853/414113 [01:10<00:20, 4584.60it/s]\u001b[A\n",
      " 77%|███████▋  | 318312/414113 [01:10<00:21, 4554.59it/s]\u001b[A\n",
      " 77%|███████▋  | 318768/414113 [01:10<00:20, 4553.63it/s]\u001b[A\n",
      " 77%|███████▋  | 319227/414113 [01:11<00:20, 4564.03it/s]\u001b[A\n",
      " 77%|███████▋  | 319692/414113 [01:11<00:20, 4588.82it/s]\u001b[A\n",
      " 77%|███████▋  | 320164/414113 [01:11<00:20, 4625.39it/s]\u001b[A\n",
      " 77%|███████▋  | 320627/414113 [01:11<00:20, 4605.86it/s]\u001b[A\n",
      " 78%|███████▊  | 321102/414113 [01:11<00:20, 4645.97it/s]\u001b[A\n",
      " 78%|███████▊  | 321567/414113 [01:11<00:20, 4591.52it/s]\u001b[A\n",
      " 78%|███████▊  | 322036/414113 [01:11<00:19, 4618.03it/s]\u001b[A\n",
      " 78%|███████▊  | 322499/414113 [01:11<00:20, 4575.68it/s]\u001b[A\n",
      " 78%|███████▊  | 322957/414113 [01:11<00:20, 4556.07it/s]\u001b[A\n",
      " 78%|███████▊  | 323413/414113 [01:11<00:19, 4551.08it/s]\u001b[A\n",
      " 78%|███████▊  | 323869/414113 [01:12<00:20, 4396.25it/s]\u001b[A\n",
      " 78%|███████▊  | 324332/414113 [01:12<00:20, 4462.59it/s]\u001b[A\n",
      " 78%|███████▊  | 324796/414113 [01:12<00:19, 4512.49it/s]\u001b[A\n",
      " 79%|███████▊  | 325253/414113 [01:12<00:19, 4529.13it/s]\u001b[A\n",
      " 79%|███████▊  | 325721/414113 [01:12<00:19, 4571.79it/s]\u001b[A\n",
      " 79%|███████▉  | 326197/414113 [01:12<00:19, 4624.51it/s]\u001b[A\n",
      " 79%|███████▉  | 326660/414113 [01:12<00:18, 4617.72it/s]\u001b[A\n",
      " 79%|███████▉  | 327123/414113 [01:12<00:18, 4593.93it/s]\u001b[A\n",
      " 79%|███████▉  | 327583/414113 [01:12<00:18, 4554.88it/s]\u001b[A\n",
      " 79%|███████▉  | 328039/414113 [01:12<00:19, 4529.12it/s]\u001b[A\n",
      " 79%|███████▉  | 328508/414113 [01:13<00:18, 4574.03it/s]\u001b[A\n",
      " 79%|███████▉  | 328966/414113 [01:13<00:18, 4549.70it/s]\u001b[A\n",
      " 80%|███████▉  | 329422/414113 [01:13<00:18, 4503.68it/s]\u001b[A\n",
      " 80%|███████▉  | 329882/414113 [01:13<00:18, 4529.80it/s]\u001b[A\n",
      " 80%|███████▉  | 330336/414113 [01:13<00:18, 4515.74it/s]\u001b[A\n",
      " 80%|███████▉  | 330788/414113 [01:13<00:18, 4425.74it/s]\u001b[A\n",
      " 80%|███████▉  | 331233/414113 [01:13<00:18, 4431.74it/s]\u001b[A\n",
      " 80%|████████  | 331677/414113 [01:13<00:18, 4418.04it/s]\u001b[A\n",
      " 80%|████████  | 332120/414113 [01:13<00:18, 4387.31it/s]\u001b[A\n",
      " 80%|████████  | 332559/414113 [01:14<00:18, 4372.86it/s]\u001b[A\n",
      " 80%|████████  | 332997/414113 [01:14<00:18, 4358.43it/s]\u001b[A\n",
      " 81%|████████  | 333447/414113 [01:14<00:18, 4398.31it/s]\u001b[A\n",
      " 81%|████████  | 333888/414113 [01:14<00:18, 4370.37it/s]\u001b[A\n",
      " 81%|████████  | 334332/414113 [01:14<00:18, 4389.12it/s]\u001b[A\n",
      " 81%|████████  | 334797/414113 [01:14<00:17, 4463.59it/s]\u001b[A\n",
      " 81%|████████  | 335244/414113 [01:14<00:17, 4463.87it/s]\u001b[A\n",
      " 81%|████████  | 335691/414113 [01:14<00:17, 4454.76it/s]\u001b[A\n",
      " 81%|████████  | 336137/414113 [01:14<00:17, 4425.08it/s]\u001b[A\n",
      " 81%|████████▏ | 336588/414113 [01:14<00:17, 4447.52it/s]\u001b[A\n",
      " 81%|████████▏ | 337039/414113 [01:15<00:17, 4464.40it/s]\u001b[A\n",
      " 81%|████████▏ | 337486/414113 [01:15<00:17, 4427.87it/s]\u001b[A\n",
      " 82%|████████▏ | 337938/414113 [01:15<00:17, 4452.28it/s]\u001b[A\n",
      " 82%|████████▏ | 338384/414113 [01:15<00:17, 4415.06it/s]\u001b[A\n",
      " 82%|████████▏ | 338826/414113 [01:15<00:17, 4380.54it/s]\u001b[A\n",
      " 82%|████████▏ | 339287/414113 [01:15<00:16, 4445.73it/s]\u001b[A\n",
      " 82%|████████▏ | 339733/414113 [01:15<00:16, 4448.18it/s]\u001b[A\n",
      " 82%|████████▏ | 340191/414113 [01:15<00:16, 4484.21it/s]\u001b[A\n",
      " 82%|████████▏ | 340640/414113 [01:15<00:16, 4460.04it/s]\u001b[A\n",
      " 82%|████████▏ | 341087/414113 [01:15<00:16, 4436.13it/s]\u001b[A\n",
      " 82%|████████▏ | 341551/414113 [01:16<00:16, 4494.40it/s]\u001b[A\n",
      " 83%|████████▎ | 342001/414113 [01:16<00:16, 4491.06it/s]\u001b[A\n",
      " 83%|████████▎ | 342451/414113 [01:16<00:15, 4483.24it/s]\u001b[A\n",
      " 83%|████████▎ | 342911/414113 [01:16<00:15, 4515.36it/s]\u001b[A\n",
      " 83%|████████▎ | 343370/414113 [01:16<00:15, 4537.39it/s]\u001b[A\n",
      " 83%|████████▎ | 343829/414113 [01:16<00:15, 4549.94it/s]\u001b[A\n",
      " 83%|████████▎ | 344285/414113 [01:16<00:15, 4534.29it/s]\u001b[A\n",
      " 83%|████████▎ | 344739/414113 [01:16<00:15, 4439.64it/s]\u001b[A\n",
      " 83%|████████▎ | 345190/414113 [01:16<00:15, 4457.32it/s]\u001b[A\n",
      " 83%|████████▎ | 345637/414113 [01:16<00:15, 4434.44it/s]\u001b[A\n",
      " 84%|████████▎ | 346084/414113 [01:17<00:15, 4443.76it/s]\u001b[A\n",
      " 84%|████████▎ | 346529/414113 [01:17<00:30, 2207.44it/s]\u001b[A\n",
      " 84%|████████▍ | 346971/414113 [01:17<00:25, 2597.35it/s]\u001b[A\n",
      " 84%|████████▍ | 347434/414113 [01:17<00:22, 2990.42it/s]\u001b[A\n",
      " 84%|████████▍ | 347887/414113 [01:17<00:19, 3329.73it/s]\u001b[A\n",
      " 84%|████████▍ | 348321/414113 [01:17<00:18, 3578.99it/s]\u001b[A\n",
      " 84%|████████▍ | 348768/414113 [01:17<00:17, 3805.00it/s]\u001b[A\n",
      " 84%|████████▍ | 349242/414113 [01:18<00:16, 4043.10it/s]\u001b[A\n",
      " 84%|████████▍ | 349703/414113 [01:18<00:15, 4197.51it/s]\u001b[A\n",
      " 85%|████████▍ | 350165/414113 [01:18<00:14, 4313.76it/s]\u001b[A\n",
      " 85%|████████▍ | 350635/414113 [01:18<00:14, 4421.53it/s]\u001b[A\n",
      " 85%|████████▍ | 351106/414113 [01:18<00:13, 4501.54it/s]\u001b[A\n",
      " 85%|████████▍ | 351567/414113 [01:18<00:13, 4517.32it/s]\u001b[A\n",
      " 85%|████████▌ | 352030/414113 [01:18<00:13, 4549.91it/s]\u001b[A\n",
      " 85%|████████▌ | 352491/414113 [01:18<00:13, 4503.07it/s]\u001b[A\n",
      " 85%|████████▌ | 352946/414113 [01:18<00:13, 4468.18it/s]\u001b[A\n",
      " 85%|████████▌ | 353396/414113 [01:18<00:13, 4449.34it/s]\u001b[A\n",
      " 85%|████████▌ | 353868/414113 [01:19<00:13, 4524.07it/s]\u001b[A\n",
      " 86%|████████▌ | 354323/414113 [01:19<00:13, 4524.08it/s]\u001b[A\n",
      " 86%|████████▌ | 354787/414113 [01:19<00:13, 4557.86it/s]\u001b[A\n",
      " 86%|████████▌ | 355244/414113 [01:19<00:12, 4553.42it/s]\u001b[A\n",
      " 86%|████████▌ | 355700/414113 [01:19<00:12, 4549.50it/s]\u001b[A\n",
      " 86%|████████▌ | 356166/414113 [01:19<00:12, 4581.67it/s]\u001b[A\n",
      " 86%|████████▌ | 356625/414113 [01:19<00:12, 4525.41it/s]\u001b[A\n",
      " 86%|████████▌ | 357078/414113 [01:19<00:12, 4457.52it/s]\u001b[A\n",
      " 86%|████████▋ | 357525/414113 [01:19<00:12, 4431.90it/s]\u001b[A\n",
      " 86%|████████▋ | 357973/414113 [01:20<00:12, 4443.75it/s]\u001b[A\n",
      " 87%|████████▋ | 358422/414113 [01:20<00:12, 4455.25it/s]\u001b[A\n",
      " 87%|████████▋ | 358898/414113 [01:20<00:12, 4541.56it/s]\u001b[A\n",
      " 87%|████████▋ | 359359/414113 [01:20<00:12, 4559.87it/s]\u001b[A\n",
      " 87%|████████▋ | 359816/414113 [01:20<00:11, 4562.41it/s]\u001b[A\n",
      " 87%|████████▋ | 360273/414113 [01:20<00:11, 4541.59it/s]\u001b[A\n",
      " 87%|████████▋ | 360728/414113 [01:20<00:11, 4504.40it/s]\u001b[A\n",
      " 87%|████████▋ | 361179/414113 [01:20<00:11, 4501.91it/s]\u001b[A\n",
      " 87%|████████▋ | 361630/414113 [01:20<00:12, 4300.91it/s]\u001b[A\n",
      " 87%|████████▋ | 362081/414113 [01:20<00:11, 4359.97it/s]\u001b[A\n",
      " 88%|████████▊ | 362523/414113 [01:21<00:11, 4375.89it/s]\u001b[A\n",
      " 88%|████████▊ | 362964/414113 [01:21<00:11, 4384.14it/s]\u001b[A\n",
      " 88%|████████▊ | 363408/414113 [01:21<00:11, 4399.75it/s]\u001b[A\n",
      " 88%|████████▊ | 363862/414113 [01:21<00:11, 4440.64it/s]\u001b[A\n",
      " 88%|████████▊ | 364312/414113 [01:21<00:11, 4455.79it/s]\u001b[A\n",
      " 88%|████████▊ | 364770/414113 [01:21<00:10, 4490.84it/s]\u001b[A\n",
      " 88%|████████▊ | 365220/414113 [01:21<00:10, 4480.79it/s]\u001b[A\n",
      " 88%|████████▊ | 365669/414113 [01:21<00:10, 4438.43it/s]\u001b[A\n",
      " 88%|████████▊ | 366114/414113 [01:21<00:10, 4430.71it/s]\u001b[A\n",
      " 89%|████████▊ | 366567/414113 [01:21<00:10, 4458.52it/s]\u001b[A\n",
      " 89%|████████▊ | 367014/414113 [01:22<00:10, 4442.71it/s]\u001b[A\n",
      " 89%|████████▊ | 367461/414113 [01:22<00:10, 4448.54it/s]\u001b[A\n",
      " 89%|████████▉ | 367920/414113 [01:22<00:10, 4486.96it/s]\u001b[A\n",
      " 89%|████████▉ | 368382/414113 [01:22<00:10, 4525.54it/s]\u001b[A\n",
      " 89%|████████▉ | 368835/414113 [01:22<00:10, 4502.49it/s]\u001b[A\n",
      " 89%|████████▉ | 369306/414113 [01:22<00:09, 4561.50it/s]\u001b[A\n",
      " 89%|████████▉ | 369763/414113 [01:22<00:09, 4539.49it/s]\u001b[A\n",
      " 89%|████████▉ | 370218/414113 [01:22<00:09, 4513.72it/s]\u001b[A\n",
      " 90%|████████▉ | 370670/414113 [01:22<00:09, 4457.32it/s]\u001b[A\n",
      " 90%|████████▉ | 371117/414113 [01:22<00:09, 4452.73it/s]\u001b[A\n",
      " 90%|████████▉ | 371563/414113 [01:23<00:09, 4408.86it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 372017/414113 [01:23<00:09, 4446.66it/s]\u001b[A\n",
      " 90%|████████▉ | 372463/414113 [01:23<00:09, 4448.79it/s]\u001b[A\n",
      " 90%|█████████ | 372924/414113 [01:23<00:09, 4494.56it/s]\u001b[A\n",
      " 90%|█████████ | 373374/414113 [01:23<00:09, 4482.38it/s]\u001b[A\n",
      " 90%|█████████ | 373842/414113 [01:23<00:08, 4539.18it/s]\u001b[A\n",
      " 90%|█████████ | 374300/414113 [01:23<00:08, 4550.69it/s]\u001b[A\n",
      " 90%|█████████ | 374756/414113 [01:23<00:08, 4547.51it/s]\u001b[A\n",
      " 91%|█████████ | 375211/414113 [01:23<00:08, 4508.25it/s]\u001b[A\n",
      " 91%|█████████ | 375663/414113 [01:23<00:08, 4489.84it/s]\u001b[A\n",
      " 91%|█████████ | 376119/414113 [01:24<00:08, 4508.22it/s]\u001b[A\n",
      " 91%|█████████ | 376570/414113 [01:24<00:08, 4500.90it/s]\u001b[A\n",
      " 91%|█████████ | 377021/414113 [01:24<00:08, 4485.77it/s]\u001b[A\n",
      " 91%|█████████ | 377470/414113 [01:24<00:08, 4430.48it/s]\u001b[A\n",
      " 91%|█████████▏| 377935/414113 [01:24<00:08, 4493.84it/s]\u001b[A\n",
      " 91%|█████████▏| 378407/414113 [01:24<00:07, 4557.57it/s]\u001b[A\n",
      " 91%|█████████▏| 378865/414113 [01:24<00:07, 4563.70it/s]\u001b[A\n",
      " 92%|█████████▏| 379322/414113 [01:24<00:07, 4505.97it/s]\u001b[A\n",
      " 92%|█████████▏| 379773/414113 [01:24<00:07, 4442.48it/s]\u001b[A\n",
      " 92%|█████████▏| 380218/414113 [01:24<00:07, 4434.16it/s]\u001b[A\n",
      " 92%|█████████▏| 380662/414113 [01:25<00:07, 4425.84it/s]\u001b[A\n",
      " 92%|█████████▏| 381110/414113 [01:25<00:07, 4441.20it/s]\u001b[A\n",
      " 92%|█████████▏| 381565/414113 [01:25<00:07, 4471.64it/s]\u001b[A\n",
      " 92%|█████████▏| 382013/414113 [01:25<00:07, 4351.40it/s]\u001b[A\n",
      " 92%|█████████▏| 382468/414113 [01:25<00:07, 4408.36it/s]\u001b[A\n",
      " 92%|█████████▏| 382937/414113 [01:25<00:06, 4488.76it/s]\u001b[A\n",
      " 93%|█████████▎| 383395/414113 [01:25<00:06, 4513.55it/s]\u001b[A\n",
      " 93%|█████████▎| 383850/414113 [01:25<00:06, 4522.11it/s]\u001b[A\n",
      " 93%|█████████▎| 384303/414113 [01:25<00:06, 4464.63it/s]\u001b[A\n",
      " 93%|█████████▎| 384750/414113 [01:25<00:06, 4448.77it/s]\u001b[A\n",
      " 93%|█████████▎| 385209/414113 [01:26<00:06, 4488.85it/s]\u001b[A\n",
      " 93%|█████████▎| 385659/414113 [01:26<00:06, 4491.61it/s]\u001b[A\n",
      " 93%|█████████▎| 386114/414113 [01:26<00:06, 4508.61it/s]\u001b[A\n",
      " 93%|█████████▎| 386572/414113 [01:26<00:06, 4527.54it/s]\u001b[A\n",
      " 93%|█████████▎| 387038/414113 [01:26<00:05, 4563.65it/s]\u001b[A\n",
      " 94%|█████████▎| 387499/414113 [01:26<00:05, 4575.43it/s]\u001b[A\n",
      " 94%|█████████▎| 387957/414113 [01:26<00:05, 4561.01it/s]\u001b[A\n",
      " 94%|█████████▍| 388414/414113 [01:26<00:05, 4562.36it/s]\u001b[A\n",
      " 94%|█████████▍| 388871/414113 [01:26<00:05, 4522.80it/s]\u001b[A\n",
      " 94%|█████████▍| 389324/414113 [01:26<00:05, 4502.89it/s]\u001b[A\n",
      " 94%|█████████▍| 389787/414113 [01:27<00:05, 4539.08it/s]\u001b[A\n",
      " 94%|█████████▍| 390242/414113 [01:27<00:05, 4534.09it/s]\u001b[A\n",
      " 94%|█████████▍| 390696/414113 [01:27<00:05, 4527.43it/s]\u001b[A\n",
      " 94%|█████████▍| 391151/414113 [01:27<00:05, 4531.49it/s]\u001b[A\n",
      " 95%|█████████▍| 391625/414113 [01:27<00:04, 4590.27it/s]\u001b[A\n",
      " 95%|█████████▍| 392085/414113 [01:27<00:04, 4580.17it/s]\u001b[A\n",
      " 95%|█████████▍| 392544/414113 [01:27<00:04, 4570.46it/s]\u001b[A\n",
      " 95%|█████████▍| 393002/414113 [01:27<00:04, 4527.64it/s]\u001b[A\n",
      " 95%|█████████▌| 393455/414113 [01:27<00:04, 4461.44it/s]\u001b[A\n",
      " 95%|█████████▌| 393902/414113 [01:28<00:04, 4329.85it/s]\u001b[A\n",
      " 95%|█████████▌| 394355/414113 [01:28<00:04, 4385.96it/s]\u001b[A\n",
      " 95%|█████████▌| 394814/414113 [01:28<00:04, 4445.04it/s]\u001b[A\n",
      " 95%|█████████▌| 395270/414113 [01:28<00:04, 4477.82it/s]\u001b[A\n",
      " 96%|█████████▌| 395724/414113 [01:28<00:04, 4495.10it/s]\u001b[A\n",
      " 96%|█████████▌| 396176/414113 [01:28<00:03, 4500.43it/s]\u001b[A\n",
      " 96%|█████████▌| 396632/414113 [01:28<00:03, 4515.42it/s]\u001b[A\n",
      " 96%|█████████▌| 397088/414113 [01:28<00:03, 4528.22it/s]\u001b[A\n",
      " 96%|█████████▌| 397542/414113 [01:28<00:03, 4503.18it/s]\u001b[A\n",
      " 96%|█████████▌| 397993/414113 [01:28<00:03, 4478.50it/s]\u001b[A\n",
      " 96%|█████████▌| 398441/414113 [01:29<00:03, 4452.05it/s]\u001b[A\n",
      " 96%|█████████▋| 398887/414113 [01:29<00:03, 4443.39it/s]\u001b[A\n",
      " 96%|█████████▋| 399332/414113 [01:29<00:03, 4432.45it/s]\u001b[A\n",
      " 97%|█████████▋| 399779/414113 [01:29<00:03, 4441.55it/s]\u001b[A\n",
      " 97%|█████████▋| 400234/414113 [01:29<00:03, 4472.73it/s]\u001b[A\n",
      " 97%|█████████▋| 400689/414113 [01:29<00:02, 4495.42it/s]\u001b[A\n",
      " 97%|█████████▋| 401139/414113 [01:29<00:02, 4457.45it/s]\u001b[A\n",
      " 97%|█████████▋| 401598/414113 [01:29<00:02, 4494.29it/s]\u001b[A\n",
      " 97%|█████████▋| 402048/414113 [01:29<00:02, 4493.22it/s]\u001b[A\n",
      " 97%|█████████▋| 402498/414113 [01:29<00:02, 4462.35it/s]\u001b[A\n",
      " 97%|█████████▋| 402958/414113 [01:30<00:02, 4502.41it/s]\u001b[A\n",
      " 97%|█████████▋| 403420/414113 [01:30<00:02, 4537.05it/s]\u001b[A\n",
      " 98%|█████████▊| 403881/414113 [01:30<00:02, 4556.68it/s]\u001b[A\n",
      " 98%|█████████▊| 404352/414113 [01:30<00:02, 4601.49it/s]\u001b[A\n",
      " 98%|█████████▊| 404813/414113 [01:30<00:02, 4544.14it/s]\u001b[A\n",
      " 98%|█████████▊| 405268/414113 [01:30<00:01, 4536.77it/s]\u001b[A\n",
      " 98%|█████████▊| 405724/414113 [01:30<00:01, 4541.03it/s]\u001b[A\n",
      " 98%|█████████▊| 406179/414113 [01:30<00:01, 4505.98it/s]\u001b[A\n",
      " 98%|█████████▊| 406630/414113 [01:30<00:01, 4461.63it/s]\u001b[A\n",
      " 98%|█████████▊| 407077/414113 [01:30<00:01, 4447.06it/s]\u001b[A\n",
      " 98%|█████████▊| 407534/414113 [01:31<00:01, 4483.08it/s]\u001b[A\n",
      " 99%|█████████▊| 407983/414113 [01:31<00:01, 4455.72it/s]\u001b[A\n",
      " 99%|█████████▊| 408439/414113 [01:31<00:01, 4485.81it/s]\u001b[A\n",
      " 99%|█████████▊| 408888/414113 [01:31<00:01, 4443.11it/s]\u001b[A\n",
      " 99%|█████████▉| 409340/414113 [01:31<00:01, 4465.71it/s]\u001b[A\n",
      " 99%|█████████▉| 409796/414113 [01:31<00:00, 4492.07it/s]\u001b[A\n",
      " 99%|█████████▉| 410259/414113 [01:31<00:00, 4529.42it/s]\u001b[A\n",
      " 99%|█████████▉| 410713/414113 [01:31<00:00, 4379.27it/s]\u001b[A\n",
      " 99%|█████████▉| 411153/414113 [01:31<00:00, 4375.95it/s]\u001b[A\n",
      " 99%|█████████▉| 411608/414113 [01:31<00:00, 4425.31it/s]\u001b[A\n",
      "100%|█████████▉| 412061/414113 [01:32<00:00, 4455.21it/s]\u001b[A\n",
      "100%|█████████▉| 412523/414113 [01:32<00:00, 4500.30it/s]\u001b[A\n",
      "100%|█████████▉| 412976/414113 [01:32<00:00, 4507.04it/s]\u001b[A\n",
      "100%|█████████▉| 413444/414113 [01:32<00:00, 4555.02it/s]\u001b[A\n",
      "100%|█████████▉| 413900/414113 [01:32<00:00, 4550.91it/s]\u001b[A\n",
      "100%|██████████| 414113/414113 [01:32<00:00, 4476.53it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 32          # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/12942], Loss: 3.8352, Perplexity: 46.3043\n",
      "Epoch [1/3], Step [200/12942], Loss: 3.5738, Perplexity: 35.6516\n",
      "Epoch [1/3], Step [300/12942], Loss: 3.7985, Perplexity: 44.6329\n",
      "Epoch [1/3], Step [400/12942], Loss: 3.3410, Perplexity: 28.2466\n",
      "Epoch [1/3], Step [500/12942], Loss: 3.2424, Perplexity: 25.5959\n",
      "Epoch [1/3], Step [600/12942], Loss: 3.2654, Perplexity: 26.1898\n",
      "Epoch [1/3], Step [700/12942], Loss: 3.6007, Perplexity: 36.6250\n",
      "Epoch [1/3], Step [800/12942], Loss: 3.5145, Perplexity: 33.5984\n",
      "Epoch [1/3], Step [900/12942], Loss: 2.9030, Perplexity: 18.2286\n",
      "Epoch [1/3], Step [1000/12942], Loss: 2.8909, Perplexity: 18.0090\n",
      "Epoch [1/3], Step [1100/12942], Loss: 2.8704, Perplexity: 17.64471\n",
      "Epoch [1/3], Step [1200/12942], Loss: 2.8959, Perplexity: 18.0993\n",
      "Epoch [1/3], Step [1300/12942], Loss: 3.0593, Perplexity: 21.3125\n",
      "Epoch [1/3], Step [1400/12942], Loss: 2.8730, Perplexity: 17.6901\n",
      "Epoch [1/3], Step [1500/12942], Loss: 3.0053, Perplexity: 20.1929\n",
      "Epoch [1/3], Step [1600/12942], Loss: 2.7854, Perplexity: 16.2064\n",
      "Epoch [1/3], Step [1700/12942], Loss: 2.8078, Perplexity: 16.5736\n",
      "Epoch [1/3], Step [1800/12942], Loss: 2.9466, Perplexity: 19.0407\n",
      "Epoch [1/3], Step [1900/12942], Loss: 2.6319, Perplexity: 13.8998\n",
      "Epoch [1/3], Step [2000/12942], Loss: 2.3324, Perplexity: 10.3028\n",
      "Epoch [1/3], Step [2100/12942], Loss: 2.7717, Perplexity: 15.9858\n",
      "Epoch [1/3], Step [2200/12942], Loss: 2.3374, Perplexity: 10.3547\n",
      "Epoch [1/3], Step [2300/12942], Loss: 2.6124, Perplexity: 13.6313\n",
      "Epoch [1/3], Step [2400/12942], Loss: 2.4314, Perplexity: 11.3749\n",
      "Epoch [1/3], Step [2500/12942], Loss: 2.5370, Perplexity: 12.6421\n",
      "Epoch [1/3], Step [2600/12942], Loss: 2.7966, Perplexity: 16.3889\n",
      "Epoch [1/3], Step [2700/12942], Loss: 2.5686, Perplexity: 13.0477\n",
      "Epoch [1/3], Step [2800/12942], Loss: 2.2970, Perplexity: 9.94439\n",
      "Epoch [1/3], Step [2900/12942], Loss: 2.5844, Perplexity: 13.2557\n",
      "Epoch [1/3], Step [3000/12942], Loss: 2.8572, Perplexity: 17.4131\n",
      "Epoch [1/3], Step [3100/12942], Loss: 2.5680, Perplexity: 13.0397\n",
      "Epoch [1/3], Step [3200/12942], Loss: 2.7569, Perplexity: 15.7505\n",
      "Epoch [1/3], Step [3300/12942], Loss: 2.5179, Perplexity: 12.4026\n",
      "Epoch [1/3], Step [3400/12942], Loss: 2.6195, Perplexity: 13.7291\n",
      "Epoch [1/3], Step [3500/12942], Loss: 2.6250, Perplexity: 13.8053\n",
      "Epoch [1/3], Step [3600/12942], Loss: 2.3348, Perplexity: 10.3271\n",
      "Epoch [1/3], Step [3700/12942], Loss: 2.4727, Perplexity: 11.85475\n",
      "Epoch [1/3], Step [3800/12942], Loss: 2.3758, Perplexity: 10.7595\n",
      "Epoch [1/3], Step [3900/12942], Loss: 2.3268, Perplexity: 10.2450\n",
      "Epoch [1/3], Step [4000/12942], Loss: 2.4054, Perplexity: 11.0830\n",
      "Epoch [1/3], Step [4100/12942], Loss: 3.4371, Perplexity: 31.0965\n",
      "Epoch [1/3], Step [4200/12942], Loss: 2.3924, Perplexity: 10.9394\n",
      "Epoch [1/3], Step [4300/12942], Loss: 2.5556, Perplexity: 12.8785\n",
      "Epoch [1/3], Step [4400/12942], Loss: 2.3021, Perplexity: 9.99488\n",
      "Epoch [1/3], Step [4500/12942], Loss: 2.2790, Perplexity: 9.76705\n",
      "Epoch [1/3], Step [4600/12942], Loss: 2.2861, Perplexity: 9.83666\n",
      "Epoch [1/3], Step [4700/12942], Loss: 2.3577, Perplexity: 10.5667\n",
      "Epoch [1/3], Step [4800/12942], Loss: 2.4059, Perplexity: 11.0889\n",
      "Epoch [1/3], Step [4900/12942], Loss: 2.4129, Perplexity: 11.1659\n",
      "Epoch [1/3], Step [5000/12942], Loss: 2.2508, Perplexity: 9.49497\n",
      "Epoch [1/3], Step [5100/12942], Loss: 2.2666, Perplexity: 9.64709\n",
      "Epoch [1/3], Step [5200/12942], Loss: 2.4722, Perplexity: 11.8485\n",
      "Epoch [1/3], Step [5300/12942], Loss: 2.1837, Perplexity: 8.87881\n",
      "Epoch [1/3], Step [5400/12942], Loss: 2.4200, Perplexity: 11.2455\n",
      "Epoch [1/3], Step [5500/12942], Loss: 2.2148, Perplexity: 9.15998\n",
      "Epoch [1/3], Step [5600/12942], Loss: 2.2520, Perplexity: 9.50647\n",
      "Epoch [1/3], Step [5700/12942], Loss: 2.9636, Perplexity: 19.36705\n",
      "Epoch [1/3], Step [5800/12942], Loss: 2.2242, Perplexity: 9.24624\n",
      "Epoch [1/3], Step [5900/12942], Loss: 2.3555, Perplexity: 10.5433\n",
      "Epoch [1/3], Step [6000/12942], Loss: 2.3227, Perplexity: 10.2032\n",
      "Epoch [1/3], Step [6100/12942], Loss: 2.6570, Perplexity: 14.2528\n",
      "Epoch [1/3], Step [6200/12942], Loss: 2.4142, Perplexity: 11.1806\n",
      "Epoch [1/3], Step [6300/12942], Loss: 2.2816, Perplexity: 9.79242\n",
      "Epoch [1/3], Step [6400/12942], Loss: 2.4939, Perplexity: 12.1081\n",
      "Epoch [1/3], Step [6500/12942], Loss: 2.0547, Perplexity: 7.80415\n",
      "Epoch [1/3], Step [6600/12942], Loss: 2.1277, Perplexity: 8.39579\n",
      "Epoch [1/3], Step [6700/12942], Loss: 2.2519, Perplexity: 9.50588\n",
      "Epoch [1/3], Step [6800/12942], Loss: 2.3693, Perplexity: 10.6904\n",
      "Epoch [1/3], Step [6900/12942], Loss: 2.7570, Perplexity: 15.7519\n",
      "Epoch [1/3], Step [7000/12942], Loss: 2.2157, Perplexity: 9.16785\n",
      "Epoch [1/3], Step [7100/12942], Loss: 2.3030, Perplexity: 10.0042\n",
      "Epoch [1/3], Step [7200/12942], Loss: 2.1795, Perplexity: 8.84186\n",
      "Epoch [1/3], Step [7300/12942], Loss: 2.2147, Perplexity: 9.15833\n",
      "Epoch [1/3], Step [7400/12942], Loss: 2.8392, Perplexity: 17.1026\n",
      "Epoch [1/3], Step [7500/12942], Loss: 2.4106, Perplexity: 11.1407\n",
      "Epoch [1/3], Step [7600/12942], Loss: 2.4324, Perplexity: 11.3863\n",
      "Epoch [1/3], Step [7700/12942], Loss: 2.4287, Perplexity: 11.3444\n",
      "Epoch [1/3], Step [7800/12942], Loss: 2.4601, Perplexity: 11.7060\n",
      "Epoch [1/3], Step [7900/12942], Loss: 2.0257, Perplexity: 7.58123\n",
      "Epoch [1/3], Step [8000/12942], Loss: 2.4473, Perplexity: 11.5569\n",
      "Epoch [1/3], Step [8100/12942], Loss: 2.0542, Perplexity: 7.80033\n",
      "Epoch [1/3], Step [8200/12942], Loss: 2.1885, Perplexity: 8.92158\n",
      "Epoch [1/3], Step [8300/12942], Loss: 2.3397, Perplexity: 10.3782\n",
      "Epoch [1/3], Step [8400/12942], Loss: 2.4011, Perplexity: 11.0351\n",
      "Epoch [1/3], Step [8500/12942], Loss: 2.1971, Perplexity: 8.99868\n",
      "Epoch [1/3], Step [8600/12942], Loss: 2.3174, Perplexity: 10.1497\n",
      "Epoch [1/3], Step [8700/12942], Loss: 2.5830, Perplexity: 13.2362\n",
      "Epoch [1/3], Step [8800/12942], Loss: 2.4531, Perplexity: 11.6239\n",
      "Epoch [1/3], Step [8900/12942], Loss: 2.3394, Perplexity: 10.3747\n",
      "Epoch [1/3], Step [9000/12942], Loss: 2.9026, Perplexity: 18.2212\n",
      "Epoch [1/3], Step [9100/12942], Loss: 2.0219, Perplexity: 7.55265\n",
      "Epoch [1/3], Step [9200/12942], Loss: 2.0596, Perplexity: 7.84316\n",
      "Epoch [1/3], Step [9300/12942], Loss: 2.9655, Perplexity: 19.4050\n",
      "Epoch [1/3], Step [9400/12942], Loss: 2.6419, Perplexity: 14.0392\n",
      "Epoch [1/3], Step [9500/12942], Loss: 2.1914, Perplexity: 8.94776\n",
      "Epoch [1/3], Step [9600/12942], Loss: 2.7937, Perplexity: 16.3412\n",
      "Epoch [1/3], Step [9700/12942], Loss: 2.1640, Perplexity: 8.70621\n",
      "Epoch [1/3], Step [9800/12942], Loss: 2.6183, Perplexity: 13.7119\n",
      "Epoch [1/3], Step [9900/12942], Loss: 2.4247, Perplexity: 11.2986\n",
      "Epoch [1/3], Step [10000/12942], Loss: 2.1047, Perplexity: 8.2047\n",
      "Epoch [1/3], Step [10100/12942], Loss: 2.2349, Perplexity: 9.34600\n",
      "Epoch [1/3], Step [10200/12942], Loss: 2.6532, Perplexity: 14.1995\n",
      "Epoch [1/3], Step [10300/12942], Loss: 2.2379, Perplexity: 9.37326\n",
      "Epoch [1/3], Step [10400/12942], Loss: 2.8734, Perplexity: 17.6978\n",
      "Epoch [1/3], Step [10500/12942], Loss: 1.9230, Perplexity: 6.84158\n",
      "Epoch [1/3], Step [10600/12942], Loss: 2.2189, Perplexity: 9.19748\n",
      "Epoch [1/3], Step [10700/12942], Loss: 1.8698, Perplexity: 6.48727\n",
      "Epoch [1/3], Step [10800/12942], Loss: 2.2566, Perplexity: 9.55049\n",
      "Epoch [1/3], Step [10900/12942], Loss: 2.2011, Perplexity: 9.03519\n",
      "Epoch [1/3], Step [11000/12942], Loss: 2.2458, Perplexity: 9.44770\n",
      "Epoch [1/3], Step [11100/12942], Loss: 2.5031, Perplexity: 12.2207\n",
      "Epoch [1/3], Step [11200/12942], Loss: 2.0513, Perplexity: 7.77798\n",
      "Epoch [1/3], Step [11300/12942], Loss: 2.2938, Perplexity: 9.91224\n",
      "Epoch [1/3], Step [11400/12942], Loss: 2.1958, Perplexity: 8.98682\n",
      "Epoch [1/3], Step [11500/12942], Loss: 2.7810, Perplexity: 16.1353\n",
      "Epoch [1/3], Step [11600/12942], Loss: 2.1053, Perplexity: 8.21001\n",
      "Epoch [1/3], Step [11700/12942], Loss: 1.9122, Perplexity: 6.76811\n",
      "Epoch [1/3], Step [11800/12942], Loss: 1.8980, Perplexity: 6.67239\n",
      "Epoch [1/3], Step [11900/12942], Loss: 2.2946, Perplexity: 9.92010\n",
      "Epoch [1/3], Step [12000/12942], Loss: 1.9906, Perplexity: 7.32029\n",
      "Epoch [1/3], Step [12100/12942], Loss: 2.7115, Perplexity: 15.0520\n",
      "Epoch [1/3], Step [12200/12942], Loss: 2.3375, Perplexity: 10.3557\n",
      "Epoch [1/3], Step [12300/12942], Loss: 2.3981, Perplexity: 11.0026\n",
      "Epoch [1/3], Step [12400/12942], Loss: 2.0514, Perplexity: 7.77876\n",
      "Epoch [1/3], Step [12500/12942], Loss: 2.3574, Perplexity: 10.5636\n",
      "Epoch [1/3], Step [12600/12942], Loss: 2.2504, Perplexity: 9.49176\n",
      "Epoch [1/3], Step [12700/12942], Loss: 2.1298, Perplexity: 8.41299\n",
      "Epoch [1/3], Step [12800/12942], Loss: 1.9916, Perplexity: 7.32701\n",
      "Epoch [1/3], Step [12900/12942], Loss: 2.1269, Perplexity: 8.38888\n",
      "Epoch [2/3], Step [100/12942], Loss: 1.9191, Perplexity: 6.8149561\n",
      "Epoch [2/3], Step [200/12942], Loss: 2.2907, Perplexity: 9.88147\n",
      "Epoch [2/3], Step [300/12942], Loss: 1.9972, Perplexity: 7.36821\n",
      "Epoch [2/3], Step [400/12942], Loss: 2.6269, Perplexity: 13.8306\n",
      "Epoch [2/3], Step [500/12942], Loss: 2.2520, Perplexity: 9.50630\n",
      "Epoch [2/3], Step [600/12942], Loss: 1.8575, Perplexity: 6.40803\n",
      "Epoch [2/3], Step [700/12942], Loss: 2.2066, Perplexity: 9.08508\n",
      "Epoch [2/3], Step [800/12942], Loss: 2.1640, Perplexity: 8.70561\n",
      "Epoch [2/3], Step [900/12942], Loss: 1.9521, Perplexity: 7.04331\n",
      "Epoch [2/3], Step [1000/12942], Loss: 2.0627, Perplexity: 7.8669\n",
      "Epoch [2/3], Step [1100/12942], Loss: 1.9903, Perplexity: 7.31743\n",
      "Epoch [2/3], Step [1200/12942], Loss: 2.0879, Perplexity: 8.06791\n",
      "Epoch [2/3], Step [1300/12942], Loss: 1.9942, Perplexity: 7.34675\n",
      "Epoch [2/3], Step [1400/12942], Loss: 2.3546, Perplexity: 10.5337\n",
      "Epoch [2/3], Step [1500/12942], Loss: 2.1760, Perplexity: 8.81148\n",
      "Epoch [2/3], Step [1600/12942], Loss: 2.0924, Perplexity: 8.10440\n",
      "Epoch [2/3], Step [1700/12942], Loss: 2.1304, Perplexity: 8.41809\n",
      "Epoch [2/3], Step [1800/12942], Loss: 2.2449, Perplexity: 9.43993\n",
      "Epoch [2/3], Step [1900/12942], Loss: 2.5052, Perplexity: 12.2460\n",
      "Epoch [2/3], Step [2000/12942], Loss: 2.0674, Perplexity: 7.90431\n",
      "Epoch [2/3], Step [2100/12942], Loss: 2.1638, Perplexity: 8.70401\n",
      "Epoch [2/3], Step [2200/12942], Loss: 1.9239, Perplexity: 6.84762\n",
      "Epoch [2/3], Step [2300/12942], Loss: 2.2555, Perplexity: 9.54053\n",
      "Epoch [2/3], Step [2400/12942], Loss: 2.1144, Perplexity: 8.28454\n",
      "Epoch [2/3], Step [2500/12942], Loss: 2.1281, Perplexity: 8.39915\n",
      "Epoch [2/3], Step [2600/12942], Loss: 2.4543, Perplexity: 11.6386\n",
      "Epoch [2/3], Step [2700/12942], Loss: 2.5450, Perplexity: 12.7433\n",
      "Epoch [2/3], Step [2800/12942], Loss: 2.0785, Perplexity: 7.99260\n",
      "Epoch [2/3], Step [2900/12942], Loss: 1.9530, Perplexity: 7.05014\n",
      "Epoch [2/3], Step [3000/12942], Loss: 2.3483, Perplexity: 10.4678\n",
      "Epoch [2/3], Step [3100/12942], Loss: 2.3354, Perplexity: 10.3332\n",
      "Epoch [2/3], Step [3200/12942], Loss: 2.0800, Perplexity: 8.00474\n",
      "Epoch [2/3], Step [3300/12942], Loss: 2.3672, Perplexity: 10.6675\n",
      "Epoch [2/3], Step [3400/12942], Loss: 2.1409, Perplexity: 8.50671\n",
      "Epoch [2/3], Step [3500/12942], Loss: 2.2446, Perplexity: 9.43638\n",
      "Epoch [2/3], Step [3600/12942], Loss: 2.8913, Perplexity: 18.0159\n",
      "Epoch [2/3], Step [3700/12942], Loss: 2.0184, Perplexity: 7.52618\n",
      "Epoch [2/3], Step [3800/12942], Loss: 1.7706, Perplexity: 5.87422\n",
      "Epoch [2/3], Step [3900/12942], Loss: 2.0824, Perplexity: 8.02382\n",
      "Epoch [2/3], Step [4000/12942], Loss: 2.3129, Perplexity: 10.1037\n",
      "Epoch [2/3], Step [4100/12942], Loss: 2.2837, Perplexity: 9.81348\n",
      "Epoch [2/3], Step [4200/12942], Loss: 1.9508, Perplexity: 7.03466\n",
      "Epoch [2/3], Step [4300/12942], Loss: 2.5732, Perplexity: 13.1075\n",
      "Epoch [2/3], Step [4400/12942], Loss: 1.8848, Perplexity: 6.58497\n",
      "Epoch [2/3], Step [4500/12942], Loss: 2.1393, Perplexity: 8.49389\n",
      "Epoch [2/3], Step [4600/12942], Loss: 1.9754, Perplexity: 7.20984\n",
      "Epoch [2/3], Step [4700/12942], Loss: 2.2632, Perplexity: 9.61424\n",
      "Epoch [2/3], Step [4800/12942], Loss: 2.1051, Perplexity: 8.20774\n",
      "Epoch [2/3], Step [4900/12942], Loss: 2.0665, Perplexity: 7.89753\n",
      "Epoch [2/3], Step [5000/12942], Loss: 2.1561, Perplexity: 8.63750\n",
      "Epoch [2/3], Step [5100/12942], Loss: 2.2242, Perplexity: 9.24658\n",
      "Epoch [2/3], Step [5200/12942], Loss: 2.1939, Perplexity: 8.97010\n",
      "Epoch [2/3], Step [5300/12942], Loss: 2.1416, Perplexity: 8.51349\n",
      "Epoch [2/3], Step [5400/12942], Loss: 2.1490, Perplexity: 8.57633\n",
      "Epoch [2/3], Step [5500/12942], Loss: 2.0549, Perplexity: 7.80571\n",
      "Epoch [2/3], Step [5600/12942], Loss: 2.1299, Perplexity: 8.41390\n",
      "Epoch [2/3], Step [5700/12942], Loss: 2.1618, Perplexity: 8.68679\n",
      "Epoch [2/3], Step [5800/12942], Loss: 2.1860, Perplexity: 8.89955\n",
      "Epoch [2/3], Step [5900/12942], Loss: 2.5482, Perplexity: 12.7843\n",
      "Epoch [2/3], Step [6000/12942], Loss: 2.2440, Perplexity: 9.43136\n",
      "Epoch [2/3], Step [6100/12942], Loss: 2.4280, Perplexity: 11.3366\n",
      "Epoch [2/3], Step [6200/12942], Loss: 2.3071, Perplexity: 10.0456\n",
      "Epoch [2/3], Step [6300/12942], Loss: 2.2491, Perplexity: 9.47903\n",
      "Epoch [2/3], Step [6400/12942], Loss: 1.7412, Perplexity: 5.70391\n",
      "Epoch [2/3], Step [6500/12942], Loss: 2.1990, Perplexity: 9.01646\n",
      "Epoch [2/3], Step [6600/12942], Loss: 1.8725, Perplexity: 6.50462\n",
      "Epoch [2/3], Step [6700/12942], Loss: 2.1630, Perplexity: 8.69723\n",
      "Epoch [2/3], Step [6800/12942], Loss: 2.1176, Perplexity: 8.31150\n",
      "Epoch [2/3], Step [6900/12942], Loss: 2.2498, Perplexity: 9.48543\n",
      "Epoch [2/3], Step [7000/12942], Loss: 1.9868, Perplexity: 7.29249\n",
      "Epoch [2/3], Step [7100/12942], Loss: 2.3007, Perplexity: 9.98127\n",
      "Epoch [2/3], Step [7200/12942], Loss: 2.3713, Perplexity: 10.7115\n",
      "Epoch [2/3], Step [7300/12942], Loss: 1.8411, Perplexity: 6.30323\n",
      "Epoch [2/3], Step [7400/12942], Loss: 2.3334, Perplexity: 10.3132\n",
      "Epoch [2/3], Step [7500/12942], Loss: 2.3159, Perplexity: 10.1342\n",
      "Epoch [2/3], Step [7600/12942], Loss: 2.2512, Perplexity: 9.49871\n",
      "Epoch [2/3], Step [7700/12942], Loss: 2.1957, Perplexity: 8.98653\n",
      "Epoch [2/3], Step [7800/12942], Loss: 2.1424, Perplexity: 8.51961\n",
      "Epoch [2/3], Step [7900/12942], Loss: 1.8919, Perplexity: 6.63211\n",
      "Epoch [2/3], Step [8000/12942], Loss: 2.6773, Perplexity: 14.5459\n",
      "Epoch [2/3], Step [8100/12942], Loss: 2.2012, Perplexity: 9.03580\n",
      "Epoch [2/3], Step [8200/12942], Loss: 2.3121, Perplexity: 10.0955\n",
      "Epoch [2/3], Step [8300/12942], Loss: 2.2637, Perplexity: 9.61847\n",
      "Epoch [2/3], Step [8400/12942], Loss: 2.1274, Perplexity: 8.39285\n",
      "Epoch [2/3], Step [8500/12942], Loss: 1.9804, Perplexity: 7.24597\n",
      "Epoch [2/3], Step [8600/12942], Loss: 1.9337, Perplexity: 6.91502\n",
      "Epoch [2/3], Step [8700/12942], Loss: 2.1862, Perplexity: 8.90105\n",
      "Epoch [2/3], Step [8800/12942], Loss: 2.0047, Perplexity: 7.42409\n",
      "Epoch [2/3], Step [8900/12942], Loss: 2.6665, Perplexity: 14.3900\n",
      "Epoch [2/3], Step [9000/12942], Loss: 2.2408, Perplexity: 9.40079\n",
      "Epoch [2/3], Step [9100/12942], Loss: 2.4255, Perplexity: 11.3081\n",
      "Epoch [2/3], Step [9200/12942], Loss: 1.7185, Perplexity: 5.57631\n",
      "Epoch [2/3], Step [9300/12942], Loss: 2.0131, Perplexity: 7.48662\n",
      "Epoch [2/3], Step [9400/12942], Loss: 1.9765, Perplexity: 7.21720\n",
      "Epoch [2/3], Step [9500/12942], Loss: 2.6703, Perplexity: 14.4443\n",
      "Epoch [2/3], Step [9600/12942], Loss: 2.1366, Perplexity: 8.47109\n",
      "Epoch [2/3], Step [9700/12942], Loss: 2.3180, Perplexity: 10.1556\n",
      "Epoch [2/3], Step [9800/12942], Loss: 2.4036, Perplexity: 11.0633\n",
      "Epoch [2/3], Step [9900/12942], Loss: 2.1301, Perplexity: 8.41570\n",
      "Epoch [2/3], Step [10000/12942], Loss: 2.8808, Perplexity: 17.8278\n",
      "Epoch [2/3], Step [10100/12942], Loss: 2.2756, Perplexity: 9.73405\n",
      "Epoch [2/3], Step [10200/12942], Loss: 2.1941, Perplexity: 8.97204\n",
      "Epoch [2/3], Step [10300/12942], Loss: 2.2574, Perplexity: 9.55858\n",
      "Epoch [2/3], Step [10400/12942], Loss: 2.0168, Perplexity: 7.51462\n",
      "Epoch [2/3], Step [10500/12942], Loss: 2.0059, Perplexity: 7.43312\n",
      "Epoch [2/3], Step [10600/12942], Loss: 2.1518, Perplexity: 8.60078\n",
      "Epoch [2/3], Step [10700/12942], Loss: 2.1596, Perplexity: 8.66802\n",
      "Epoch [2/3], Step [10800/12942], Loss: 2.0380, Perplexity: 7.67547\n",
      "Epoch [2/3], Step [10900/12942], Loss: 2.4744, Perplexity: 11.8741\n",
      "Epoch [2/3], Step [11000/12942], Loss: 2.4447, Perplexity: 11.5269\n",
      "Epoch [2/3], Step [11100/12942], Loss: 2.1445, Perplexity: 8.53801\n",
      "Epoch [2/3], Step [11200/12942], Loss: 2.4299, Perplexity: 11.3582\n",
      "Epoch [2/3], Step [11300/12942], Loss: 1.9594, Perplexity: 7.09547\n",
      "Epoch [2/3], Step [11400/12942], Loss: 1.9981, Perplexity: 7.37501\n",
      "Epoch [2/3], Step [11500/12942], Loss: 1.8609, Perplexity: 6.42968\n",
      "Epoch [2/3], Step [11600/12942], Loss: 1.8343, Perplexity: 6.26101\n",
      "Epoch [2/3], Step [11700/12942], Loss: 1.8923, Perplexity: 6.63459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [11800/12942], Loss: 2.3949, Perplexity: 10.9668\n",
      "Epoch [2/3], Step [11900/12942], Loss: 2.2189, Perplexity: 9.19737\n",
      "Epoch [2/3], Step [12000/12942], Loss: 2.0009, Perplexity: 7.39565\n",
      "Epoch [2/3], Step [12100/12942], Loss: 2.1695, Perplexity: 8.75424\n",
      "Epoch [2/3], Step [12200/12942], Loss: 2.1818, Perplexity: 8.86255\n",
      "Epoch [2/3], Step [12300/12942], Loss: 1.9804, Perplexity: 7.24538\n",
      "Epoch [2/3], Step [12400/12942], Loss: 1.9317, Perplexity: 6.90098\n",
      "Epoch [2/3], Step [12500/12942], Loss: 1.9335, Perplexity: 6.91392\n",
      "Epoch [2/3], Step [12600/12942], Loss: 2.2567, Perplexity: 9.55119\n",
      "Epoch [2/3], Step [12700/12942], Loss: 1.9950, Perplexity: 7.35217\n",
      "Epoch [2/3], Step [12800/12942], Loss: 2.1923, Perplexity: 8.95617\n",
      "Epoch [2/3], Step [12900/12942], Loss: 1.9742, Perplexity: 7.20116\n",
      "Epoch [3/3], Step [100/12942], Loss: 2.0324, Perplexity: 7.6322933\n",
      "Epoch [3/3], Step [200/12942], Loss: 2.1646, Perplexity: 8.71101\n",
      "Epoch [3/3], Step [300/12942], Loss: 1.9433, Perplexity: 6.98159\n",
      "Epoch [3/3], Step [400/12942], Loss: 2.1862, Perplexity: 8.90174\n",
      "Epoch [3/3], Step [500/12942], Loss: 1.9978, Perplexity: 7.37303\n",
      "Epoch [3/3], Step [600/12942], Loss: 2.4813, Perplexity: 11.9573\n",
      "Epoch [3/3], Step [700/12942], Loss: 2.3222, Perplexity: 10.1982\n",
      "Epoch [3/3], Step [800/12942], Loss: 1.9002, Perplexity: 6.68705\n",
      "Epoch [3/3], Step [900/12942], Loss: 2.1213, Perplexity: 8.34211\n",
      "Epoch [3/3], Step [1000/12942], Loss: 2.4493, Perplexity: 11.5802\n",
      "Epoch [3/3], Step [1100/12942], Loss: 2.1389, Perplexity: 8.48979\n",
      "Epoch [3/3], Step [1200/12942], Loss: 1.8918, Perplexity: 6.63141\n",
      "Epoch [3/3], Step [1300/12942], Loss: 2.2934, Perplexity: 9.90838\n",
      "Epoch [3/3], Step [1400/12942], Loss: 1.7945, Perplexity: 6.01623\n",
      "Epoch [3/3], Step [1500/12942], Loss: 1.9080, Perplexity: 6.73970\n",
      "Epoch [3/3], Step [1600/12942], Loss: 1.5384, Perplexity: 4.65720\n",
      "Epoch [3/3], Step [1700/12942], Loss: 2.0327, Perplexity: 7.63450\n",
      "Epoch [3/3], Step [1800/12942], Loss: 1.9455, Perplexity: 6.99723\n",
      "Epoch [3/3], Step [1900/12942], Loss: 2.1780, Perplexity: 8.82908\n",
      "Epoch [3/3], Step [2000/12942], Loss: 2.2064, Perplexity: 9.08326\n",
      "Epoch [3/3], Step [2100/12942], Loss: 2.1024, Perplexity: 8.18608\n",
      "Epoch [3/3], Step [2200/12942], Loss: 2.2266, Perplexity: 9.26828\n",
      "Epoch [3/3], Step [2300/12942], Loss: 2.7436, Perplexity: 15.5435\n",
      "Epoch [3/3], Step [2400/12942], Loss: 1.9626, Perplexity: 7.11806\n",
      "Epoch [3/3], Step [2500/12942], Loss: 2.1608, Perplexity: 8.67811\n",
      "Epoch [3/3], Step [2600/12942], Loss: 1.8648, Perplexity: 6.45498\n",
      "Epoch [3/3], Step [2700/12942], Loss: 2.0779, Perplexity: 7.98753\n",
      "Epoch [3/3], Step [2800/12942], Loss: 1.7823, Perplexity: 5.94329\n",
      "Epoch [3/3], Step [2900/12942], Loss: 1.9758, Perplexity: 7.21266\n",
      "Epoch [3/3], Step [3000/12942], Loss: 2.0789, Perplexity: 7.99566\n",
      "Epoch [3/3], Step [3100/12942], Loss: 2.0768, Perplexity: 7.97938\n",
      "Epoch [3/3], Step [3200/12942], Loss: 2.1790, Perplexity: 8.83774\n",
      "Epoch [3/3], Step [3300/12942], Loss: 2.1939, Perplexity: 8.97059\n",
      "Epoch [3/3], Step [3400/12942], Loss: 1.9012, Perplexity: 6.69401\n",
      "Epoch [3/3], Step [3500/12942], Loss: 1.9603, Perplexity: 7.10171\n",
      "Epoch [3/3], Step [3600/12942], Loss: 2.4738, Perplexity: 11.8673\n",
      "Epoch [3/3], Step [3700/12942], Loss: 2.1726, Perplexity: 8.78090\n",
      "Epoch [3/3], Step [3800/12942], Loss: 2.2705, Perplexity: 9.684655\n",
      "Epoch [3/3], Step [3900/12942], Loss: 2.3483, Perplexity: 10.4673\n",
      "Epoch [3/3], Step [4000/12942], Loss: 2.3000, Perplexity: 9.97381\n",
      "Epoch [3/3], Step [4100/12942], Loss: 2.0888, Perplexity: 8.07507\n",
      "Epoch [3/3], Step [4200/12942], Loss: 2.1329, Perplexity: 8.43931\n",
      "Epoch [3/3], Step [4300/12942], Loss: 2.0169, Perplexity: 7.51471\n",
      "Epoch [3/3], Step [4400/12942], Loss: 2.1836, Perplexity: 8.87813\n",
      "Epoch [3/3], Step [4500/12942], Loss: 2.1288, Perplexity: 8.40485\n",
      "Epoch [3/3], Step [4600/12942], Loss: 2.2894, Perplexity: 9.86922\n",
      "Epoch [3/3], Step [4700/12942], Loss: 2.0661, Perplexity: 7.89432\n",
      "Epoch [3/3], Step [4800/12942], Loss: 1.8899, Perplexity: 6.61898\n",
      "Epoch [3/3], Step [4900/12942], Loss: 3.0386, Perplexity: 20.8768\n",
      "Epoch [3/3], Step [5000/12942], Loss: 1.8452, Perplexity: 6.32920\n",
      "Epoch [3/3], Step [5100/12942], Loss: 1.9327, Perplexity: 6.90854\n",
      "Epoch [3/3], Step [5200/12942], Loss: 2.5812, Perplexity: 13.2132\n",
      "Epoch [3/3], Step [5300/12942], Loss: 1.8736, Perplexity: 6.51203\n",
      "Epoch [3/3], Step [5400/12942], Loss: 2.1808, Perplexity: 8.85386\n",
      "Epoch [3/3], Step [5500/12942], Loss: 2.0051, Perplexity: 7.42666\n",
      "Epoch [3/3], Step [5600/12942], Loss: 2.1674, Perplexity: 8.73577\n",
      "Epoch [3/3], Step [5700/12942], Loss: 2.0520, Perplexity: 7.78322\n",
      "Epoch [3/3], Step [5800/12942], Loss: 1.8416, Perplexity: 6.30683\n",
      "Epoch [3/3], Step [5900/12942], Loss: 2.0592, Perplexity: 7.83957\n",
      "Epoch [3/3], Step [6000/12942], Loss: 2.0140, Perplexity: 7.49292\n",
      "Epoch [3/3], Step [6100/12942], Loss: 1.9496, Perplexity: 7.02611\n",
      "Epoch [3/3], Step [6200/12942], Loss: 1.9110, Perplexity: 6.76001\n",
      "Epoch [3/3], Step [6300/12942], Loss: 1.8477, Perplexity: 6.34510\n",
      "Epoch [3/3], Step [6400/12942], Loss: 1.9706, Perplexity: 7.17479\n",
      "Epoch [3/3], Step [6500/12942], Loss: 1.9535, Perplexity: 7.05364\n",
      "Epoch [3/3], Step [6600/12942], Loss: 1.8156, Perplexity: 6.14453\n",
      "Epoch [3/3], Step [6700/12942], Loss: 1.8004, Perplexity: 6.05184\n",
      "Epoch [3/3], Step [6800/12942], Loss: 2.7815, Perplexity: 16.1440\n",
      "Epoch [3/3], Step [6900/12942], Loss: 2.0029, Perplexity: 7.41024\n",
      "Epoch [3/3], Step [7000/12942], Loss: 2.0694, Perplexity: 7.92041\n",
      "Epoch [3/3], Step [7100/12942], Loss: 2.1304, Perplexity: 8.41852\n",
      "Epoch [3/3], Step [7200/12942], Loss: 2.0122, Perplexity: 7.48008\n",
      "Epoch [3/3], Step [7300/12942], Loss: 2.0520, Perplexity: 7.78352\n",
      "Epoch [3/3], Step [7400/12942], Loss: 2.4331, Perplexity: 11.39410\n",
      "Epoch [3/3], Step [7500/12942], Loss: 1.9100, Perplexity: 6.75318\n",
      "Epoch [3/3], Step [7600/12942], Loss: 1.7937, Perplexity: 6.01143\n",
      "Epoch [3/3], Step [7700/12942], Loss: 2.3733, Perplexity: 10.7329\n",
      "Epoch [3/3], Step [7800/12942], Loss: 1.9344, Perplexity: 6.92024\n",
      "Epoch [3/3], Step [7900/12942], Loss: 2.0282, Perplexity: 7.60072\n",
      "Epoch [3/3], Step [8000/12942], Loss: 2.1210, Perplexity: 8.33963\n",
      "Epoch [3/3], Step [8100/12942], Loss: 2.2731, Perplexity: 9.709869\n",
      "Epoch [3/3], Step [8200/12942], Loss: 1.9634, Perplexity: 7.12343\n",
      "Epoch [3/3], Step [8300/12942], Loss: 2.3972, Perplexity: 10.9923\n",
      "Epoch [3/3], Step [8400/12942], Loss: 1.9312, Perplexity: 6.89770\n",
      "Epoch [3/3], Step [8500/12942], Loss: 1.8799, Perplexity: 6.55293\n",
      "Epoch [3/3], Step [8600/12942], Loss: 2.0696, Perplexity: 7.92134\n",
      "Epoch [3/3], Step [8700/12942], Loss: 1.8830, Perplexity: 6.57313\n",
      "Epoch [3/3], Step [8800/12942], Loss: 1.9341, Perplexity: 6.91804\n",
      "Epoch [3/3], Step [8900/12942], Loss: 2.5226, Perplexity: 12.4614\n",
      "Epoch [3/3], Step [9000/12942], Loss: 2.2984, Perplexity: 9.95853\n",
      "Epoch [3/3], Step [9100/12942], Loss: 2.3935, Perplexity: 10.9517\n",
      "Epoch [3/3], Step [9200/12942], Loss: 2.2654, Perplexity: 9.63520\n",
      "Epoch [3/3], Step [9300/12942], Loss: 2.0323, Perplexity: 7.63200\n",
      "Epoch [3/3], Step [9400/12942], Loss: 1.8682, Perplexity: 6.47696\n",
      "Epoch [3/3], Step [9500/12942], Loss: 1.8698, Perplexity: 6.48687\n",
      "Epoch [3/3], Step [9600/12942], Loss: 2.2308, Perplexity: 9.30725\n",
      "Epoch [3/3], Step [9700/12942], Loss: 1.9921, Perplexity: 7.33088\n",
      "Epoch [3/3], Step [9800/12942], Loss: 1.9252, Perplexity: 6.85667\n",
      "Epoch [3/3], Step [9900/12942], Loss: 1.9102, Perplexity: 6.75472\n",
      "Epoch [3/3], Step [10000/12942], Loss: 2.1271, Perplexity: 8.3909\n",
      "Epoch [3/3], Step [10100/12942], Loss: 2.0002, Perplexity: 7.39076\n",
      "Epoch [3/3], Step [10200/12942], Loss: 2.0140, Perplexity: 7.49338\n",
      "Epoch [3/3], Step [10300/12942], Loss: 2.3993, Perplexity: 11.0149\n",
      "Epoch [3/3], Step [10400/12942], Loss: 2.1063, Perplexity: 8.21752\n",
      "Epoch [3/3], Step [10500/12942], Loss: 2.0882, Perplexity: 8.07078\n",
      "Epoch [3/3], Step [10600/12942], Loss: 1.8339, Perplexity: 6.25859\n",
      "Epoch [3/3], Step [10700/12942], Loss: 1.6498, Perplexity: 5.205922\n",
      "Epoch [3/3], Step [10800/12942], Loss: 1.7680, Perplexity: 5.85905\n",
      "Epoch [3/3], Step [10900/12942], Loss: 1.7031, Perplexity: 5.49078\n",
      "Epoch [3/3], Step [11000/12942], Loss: 1.9717, Perplexity: 7.18326\n",
      "Epoch [3/3], Step [11100/12942], Loss: 2.2545, Perplexity: 9.53073\n",
      "Epoch [3/3], Step [11200/12942], Loss: 1.8600, Perplexity: 6.42399\n",
      "Epoch [3/3], Step [11300/12942], Loss: 1.9356, Perplexity: 6.92844\n",
      "Epoch [3/3], Step [11400/12942], Loss: 2.0958, Perplexity: 8.13230\n",
      "Epoch [3/3], Step [11500/12942], Loss: 1.8879, Perplexity: 6.60584\n",
      "Epoch [3/3], Step [11600/12942], Loss: 2.7112, Perplexity: 15.0477\n",
      "Epoch [3/3], Step [11700/12942], Loss: 2.2184, Perplexity: 9.19290\n",
      "Epoch [3/3], Step [11800/12942], Loss: 1.8505, Perplexity: 6.36322\n",
      "Epoch [3/3], Step [11900/12942], Loss: 2.0866, Perplexity: 8.05789\n",
      "Epoch [3/3], Step [12000/12942], Loss: 1.7559, Perplexity: 5.78879\n",
      "Epoch [3/3], Step [12100/12942], Loss: 2.0675, Perplexity: 7.90511\n",
      "Epoch [3/3], Step [12200/12942], Loss: 1.7916, Perplexity: 5.99911\n",
      "Epoch [3/3], Step [12300/12942], Loss: 2.5236, Perplexity: 12.4739\n",
      "Epoch [3/3], Step [12400/12942], Loss: 2.1360, Perplexity: 8.46592\n",
      "Epoch [3/3], Step [12488/12942], Loss: 2.0382, Perplexity: 7.67663"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
